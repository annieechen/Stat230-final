---
title: "anova_and_logistic"
author: "Veena Advani"
date: "5/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### One-way ANOVA Test

Based on the predictors that were significant from part (3) above, we will perform one-way ANOVA tests with each the categorical predictors to see if there are statistically different variances in mean `sat_avg` across different categories.  We will also graph these variables against our job satisfaction (sat_avg) variable to get a better idea of the relationship between those predictors and our response variable.  Based on the analysis we've done so far, some potential candidates for predictors are collab, country, prep_A, prep_B, prep_C, gender, age, total_time, and years_teacher.

```{r}
# Collab vs Sat_avg
anova(lm(sat_avg ~ as.numeric(collab), data=teacher_data))
teacher_data %>% filter(!is.na(collab)) %>% ggplot(aes(y=sat_avg, x=as.numeric(collab))) + geom_point() + xlab("Collab") + ylab("Sat")

# prep_A vs Sat_avg
anova(lm(sat_avg ~ as.numeric(prep_A), data=teacher_data))
teacher_data %>% filter(!is.na(prep_A)) %>% ggplot(aes(y=sat_avg, x=as.numeric(prep_A))) + geom_point() + xlab("prep_A") + ylab("Sat")

# prep_B vs Sat_avg
anova(lm(sat_avg ~ as.numeric(prep_B), data=teacher_data))
teacher_data %>% filter(!is.na(prep_B)) %>% ggplot(aes(y=sat_avg, x=as.numeric(prep_B))) + geom_point() + xlab("prep_B") + ylab("Sat")

# prep_C vs Sat_avg
anova(lm(sat_avg ~ as.numeric(prep_C), data=teacher_data))
teacher_data %>% filter(!is.na(prep_C)) %>% ggplot(aes(y=sat_avg, x=as.numeric(prep_C))) + geom_point() + xlab("prep_C") + ylab("Sat")

# gender vs Sat_avg
anova(lm(sat_avg ~ gender, data=teacher_data))
teacher_data %>% filter(!is.na(gender)) %>% ggplot(aes(y=sat_avg, x=gender)) + geom_point() + xlab("Gender") + ylab("Sat")

# age vs Sat_avg
anova(lm(sat_avg ~ as.numeric(age), data=teacher_data))
teacher_data %>% filter(!is.na(age)) %>% ggplot(aes(y=sat_avg, x=as.numeric(age))) + geom_point() + xlab("Age") + ylab("Sat")

# total_time vs Sat_avg
anova(lm(sat_avg ~ as.numeric(total_time), data=teacher_data))
teacher_data %>% filter(!is.na(total_time)) %>% ggplot(aes(y=sat_avg, x=as.numeric(total_time))) + geom_point() + xlab("Total Time Working") + ylab("Sat")

# years_teacher vs Sat_avg
anova(lm(sat_avg ~ as.numeric(years_teacher), data=teacher_data))
teacher_data %>% filter(!is.na(years_teacher)) %>% ggplot(aes(y=sat_avg, x=as.numeric(years_teacher))) + geom_point() + xlab("Years As a Teacher") + ylab("Sat")
```

#### Logistic Regression Analysis

In addition to using linear regression, we will also then convert our job satisfaction variable into a categorical variable with 2 categories, "satisfied" and "unsatisfied".  We will then see if Logistic Regression yields different results than our Linear Regression approach.

For Logistic Regression we will:

(1) Try a full model, with all the predictors.
(2) Try forward, backward and bidirectional stepwise selection.

Then we will analyze the models from (1) and (2) to try to determine which columns are the best predictors of whether or not teachers are satisfied.  We will then compare these results to the results we got from linear regression.

```{r, eval=FALSE, include=FALSE}
teacher_data$isSatisfied <- teacher_data$sat_avg > 2.5
# remove NA rows, since they don't contain meaningful information for us.
teacher_data <- teacher_data[!is.na(teacher_data$sat_avg),]
# remove SAT questions, since they obviously explain our predictor variable
satcols <- grep("sat", colnames(teacher_data))
sat_indices <- satcols[satcols != which(colnames(teacher_data) == "sat_avg")]
subject_indices <- c(which(colnames(teacher_data) == "R_WR_Lit"),
	which(colnames(teacher_data) == "Math"),
	which(colnames(teacher_data) == "Science"),
	which(colnames(teacher_data) == "Soc_Studies"),
	which(colnames(teacher_data) == "Modern_Lang"),
	which(colnames(teacher_data) == "Ancient_Lang"),
	which(colnames(teacher_data) == "Tech"),
	which(colnames(teacher_data) == "Arts"),
	which(colnames(teacher_data) == "PE"),
	which(colnames(teacher_data) == "Rel_Ethics"),
	which(colnames(teacher_data) == "Practical"),
	which(colnames(teacher_data) == "Other"))
teacher_data <- teacher_data[, -c(sat_indices, subject_indices)]
# Don't use country
teacher_data <- teacher_data[,-which(colnames(teacher_data) == "country")]
# Split into train and test
set.seed(365)
train_indices <- sample(nrow(teacher_data), .8 * nrow(teacher_data))
train <- na.omit(teacher_data[train_indices,])
test <- teacher_data[-train_indices,]

m1 <- glm(isSatisfied ~., data=train)
p1.preds <- ifelse(predict(m1, newdata=test, type="response") > 0.5,1,0)
mean(p1.preds != test$isSatisfied, na.rm = TRUE)
# see what our misclassification rate would be if we asigned randomly
fake <- sample(test$isSatisfied, length(test$isSatisfied), replace=TRUE)
mean(fake != test$isSatisfied, na.rm = TRUE)

# knn.preds <- knn(train[,1:13], na.omit(test[,1:13]), train$isSatisfied, k=10)
```
