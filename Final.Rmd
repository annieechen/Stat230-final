---
title: "STAT 230 Final Project"
author: "Veena Advani, Annie Chen, Robert Tung"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library("foreign")
library("tidyr")
library("countrycode")
library("rworldmap")
library(ggplot2)
library(reshape2)
library(grid)
library(gridExtra)
library(leaps)
library(GGally)
library(ggmosaic)
library(plyr)
library("dplyr")
```

```{r, include=FALSE}
# To start, let's read in the cleaned datasets from our last submission:
teacher_data <- read.csv("TALIS.csv")

# Change column names of 10 satisfaction questions to make more readable
satcols <- grep("TT2G46", colnames(teacher_data))
indices <- satcols[satcols != which(colnames(teacher_data) == "TT2G46_avg")]
colnames(teacher_data)[indices] <- c("sat1", "sat2", "sat3", "sat4", "sat5", "sat6", "sat7", "sat8", "sat9", "sat10")
names(teacher_data)[names(teacher_data)=="TT2G46_avg"] <- "sat_avg"

# Column "TT2G15" contains no useful data, since it's all NAs, so we're going to remove it
# Column "ID" is no longer necessary, was used for making the subject dataset
teacher_data <- teacher_data[,-which(names(teacher_data) %in% c("TT2G15","ID"))]

# Change some of the subject names (one of the comments in our dataset submission was that they weren't clear):
names(teacher_data)[names(teacher_data) == "For_Lang"] <- "Modern_Lang"
names(teacher_data)[names(teacher_data) == "Gk_Ln"] <- "Ancient_Lang"

# We also thought it might be useful to have a variable that indicates the number of subjects a given teacher teaches, so let's add it below:
subject_names <- c("R_WR_Lit", "Math", "Science", "Soc_Studies", "Modern_Lang", "Ancient_Lang", "Tech", "Arts", "PE", "Rel_Ethics", "Practical", "Other")
subject_indices <- c(which(colnames(teacher_data) %in% subject_names))

teacher_data$num_subjects <- rowSums(teacher_data[,subject_indices], na.rm=TRUE)

# We discussed in our dataset submission why we weren't going to use their teacher satisfaction variable, and we are instead going to use the average of the 10 satisfaction questions, as a result, let's remove the teacher satisfaction variable:
teacher_data <- teacher_data[, -which(names(teacher_data) %in% c("satisfaction"))]

# A lot of the variables stored as categorical variables with the categories "Strongly Disagree", "Disagree", "Strongly Agree", and "Agree", are currently factors.  Let's change the ordering to go from low to high.
teacher_data$prep_A <- factor(teacher_data$prep_A, levels=c("Not at all", "Somewhat", "Well", "Very well"))
teacher_data$prep_B <- factor(teacher_data$prep_B, levels=c("Not at all", "Somewhat", "Well", "Very well"))
teacher_data$prep_C <- factor(teacher_data$prep_C, levels=c("Not at all", "Somewhat", "Well", "Very well"))
teacher_data$feedback_salary <- factor(teacher_data$feedback_salary, levels=c("No positive change", "A small change", "A moderate change", "A large change"))
teacher_data$collab <- factor(teacher_data$collab, levels=c("Strongly disagree", "Disagree", "Agree", "Strongly agree"))

# We'll split the country data into regions using the World Health Organization  categories, so that we can try to find regional patterns

# Brazil, Chile
teacher_data$region[teacher_data$country %in% c("BRA", "CHL")] <- "South America"

# Canada, Mexico, United States
teacher_data$region[teacher_data$country %in% c("CAB", "MEX", "USA")] <- "North America"

#  Bulgaria, Czech Republic,  Poland,  Romania, Russia, Slovak Republic
teacher_data$region[teacher_data$country %in% 
                        c("BGR", "CZE", "POL", "ROU", "RUS", "SVK")] <- "Eastern Europe"

# Denmark, England, Estonia, Finland,   Latvia,  Norway, Sweden
teacher_data$region[teacher_data$country %in% 
                        c("DNK", "ENG",  "EST", "FIN", "LVA", "NOR", "SWE")] <- "Northern Europe"

# Spain, Croatia, Italy, Latvia,  Portugal,  Serbia, 
teacher_data$region[teacher_data$country %in% 
                        c("ESP", "HRV", "ITA", "LVA", "PRT", "SRB")] <- "Southern Europe"

# Belgium,  France, Netherlands
teacher_data$region[teacher_data$country %in% 
                        c("BFL", "FRA","NLD")] <- "Western Europe"

# Abu Dhabi, Georgia, Israel                     
teacher_data$region[teacher_data$country %in% c("AAD", "GEO", "ISR")] <- "East Mediterranean"

# Australia, China, Japan, South Korea, Malaysia, New Zealand, Singapore
teacher_data$region[teacher_data$country %in% c("AUS", "CSH", "JPN", "KOR", "MYS", "NZL", "SGP")] <- "Western Pacific"

teacher_data$region <- as.factor(teacher_data$region)
```

### Introduction
  This analysis will focus on the Teaching and Learning International Survey (TALIS) Dataset, coordinated by the Organization for Economic Cooperation and Development (OECD), an intergovernmental organization of industrialized countries. The TALIS dataset comes from a survey of the teaching workforce that aims to collect information about teaching as a profession, the working conditions of teachers, and the learning environments of schools. It also aimed to provide a more global view of education systems, and metrics that could be used to compare education systems from different countries. TALIS was administered in 2008 and 2013. We are using the dataset from 2013, which contains answers from teachers from more than 10,000 schools in 36 different industrialized countries. Not all of the countries were English speaking. The TALIS dataset contains several different files, which are split by school type. Although the dataset contains information from a variety of school types (primary, secondary, etc), most of the data was on lower-secondary schools, so we focused on that file. At each school that was selected to participate, the principal and a random selection of up to 22 teachers were chosen to complete voluntary online questionnaires. We only analyzed teacher responses.
  
  (1) Our 1st question is: Which variables and classroom factors are the best predictors of teacher job satisfaction?
  
  There are several reasons motivating this question. Education is incredibly important to any country's development and functioning. However, countries vary in how much the teaching profession is respected, and how well education systems are funded.  As a result, teacher working conditions differ across countries. It's helpful to understand what kinds of environments keep teachers more satisfied, so that educational systems can encourage and build these types of environments. If teacher satisfaction is too low, it becomes difficult to retain a large enough teaching force, and to attract talented people to the field. Therefore, we want to understand how teacher satisfaction varies across the 36 countries in the data set, and which aspects of a school and environment correlate most with teacher satisfaction.
  
  (2) Our 2nd question is: How do teacher satisfaction, and some of the significant predictors found from question 1, vary by country?

Because the TALIS dataset allows cross country comparison, we wanted to look at how some of the variables in the dataset varied by country. Looking at how teacher satisfaction varies by region could be particularly helpful to educators and policy makers. There are a lot of environmental factors that can affect teacher satisfaction, and they might not all be addressed in the TALIS questionnaire.  However, if we can identify which countries have particularly satisfied teachers, then perhaps people who are looking to increase teacher satisfaction can further research how schools work in those countries, past the information covered in the questionnaire. Additionally, if we are able to identify any variables that are particularly helpful in predicting teacher satisfaction (from research question 1), we want to see if the countries that have a higher average teacher satisfaction, also have values for those significant variables. Or in other words, after identifying qualities that seem to correlate with high teacher satisfaction, do the more satisfied countries seem to have those qualitites? Or does it seem like other factors, that are possibly not in the data set, are contributing to the higher levels of teacher satisfaction in those countries?

### Data

Each row of the dataset corresponds to single teacher's response to the questionnaire (There are 117876 rows).  The original dataset had 532 columns, which corresponded to either a particular question in the questionnaire, or a summary variable for a section of the questionnaire. There are columns concerning employment status, personal information/background, time spent, professional development, efficacy and amount of feedback, outlook about the school, school climate, overall job satisfaction, and much more. Almost all of the questions in the questionnaire were multiple choice or numerical input questions.

After picking a subset of those columns and creating some of our own, we were left with 40 columns, resulting in a dataset that has 117876 rows and 40 columns. The variables we are using are:

*`country (CNTRY):` Country of teacher (factor variable, 36 levels)
*`region:` Region the teacher's country is in, based on World Health Organization regions (factor variable, 8 levels)
*`gender (TT2G01):` Teacher Gender (factor variable, 2 levels)
*`age (TT2G02):` Teacher Age (numeric variable)
*`years_teacher (TT2G05B):` Years of teaching experience (numeric variable)
*`age_ratio:` years_teacher divided by age, or in other words, fraction of life spent teaching (numeric variable)
*`train_stat (TT2G11):` Training level of teacher (factor variable)
*`prep_A/B/C (TT2G13A/B/C):` How prepared the teacher felt with respect to 3 different areas of teaching, which were content, pedagogy and classroom practice respectively (ordered factor variable, 4 levels)
*`avg_prep:` The average of each teachers responses for prep_A/B/C (numeric variable between 1 and 4)
*`total_time (TT2G16):` Total time spent on teaching related activities in hours per week (numeric variable)
*`feedback_salary (TT2G30G):` Whether performance feedback influences salary (ordered factor variable, 4 levels)
* `collab (TT2G44E):` How collaborative is the school culture? (ordered factor variable, 4 levels)
* `num_students (TT2G38):` Number of students in the teacher’s class (numeric variable)
* `satisfaction (TJOBSATS):` Self rated teacher job satisfaction (ordered factor variable, 4 levels)
* `Various Subject Names (TT2G15[A - L]):` 12 columns representing which subjects the teacher teaches (boolean numeric variable)
* `num_subjects:` The number of subjects the teacher teaches (numeric variable)
* `sat1-sat10 (TT2G46[A - J]):` Ten questions related to teacher job satisfaction (ordered factor variables, 4 levels)
* `sat_avg:` An average of the ten questions related to teacher job satisfaction (numeric variable).  This is our repsonse variable for research question 1.

##### Initial Plots Describing Data

Below are three plots that show our data and hopefully motivate our research questions and the analysis.

```{r, warning=FALSE, echo=FALSE}
# Histogram of teacher satisfaction
p1 <- teacher_data %>% ggplot(aes(x=sat_avg)) + geom_histogram(binwidth = 0.2) + labs(title="Plot 1: Histogram of Teacher Satisfaction", x="Teacher Satisfaction (low to high)", y="Count")

# Boxplot of whether they consider their school a collaborative environment to satisfaction
p2 <- teacher_data %>% filter(!is.na(collab)) %>% ggplot(aes(x=collab, y=sat_avg)) + geom_boxplot() +
  labs(title="Plot 2: Relationship Between Collaboration Level
       of the School and Teacher Satisfaction",
       x="Response to 'There is a collaborative school culture
       which is characterised by mutual support'",
       y="Teacher Satisfaction")

# Boxplot of region to satisfaction
p3 <- teacher_data %>% filter(!is.na(sat_avg)) %>% ggplot(aes(x=reorder(region, -sat_avg, median), y=sat_avg)) + geom_boxplot() + labs(title="Plot 3: Teacher Satisfaction by Region", x="Region", y="Teacher Satisfaction")
```

```{r, echo=FALSE, fig.height=2.25, fig.width=10, message=FALSE, warning=FALSE}
grid.arrange(p1, p2, ncol=2)
```

```{r, echo=FALSE, fig.height=2.25, fig.width=9}
p3
```

**Plot 1** As we are later analyzing the effect of different variables on teacher satisfaction, and the variation between different regions, it is important to note the overall distribution of our response variable. 

**Plot 2** provides a box plot of collab vs sat_avg. This plot helps  answerour first research question, in which we assess which variables are particularly predictive of overall teacher satisfaction. We can see a strong visual trend between how collaborative a teacher viewed his/her school environment to be, and how satisfied that teacher was. We will provide similar plots for other variables in the analysis of our first research question.

**Plot 3** provides a box plot of geographic region vs sat_avg. This is relevant to our 2nd question, which looks at variations in sat_avg and other variables across geographical regions. Visually, while all median values for sat_avg hover around the range of 2.5 to 3.5, there is still a noticeable difference in some of the distributions (e.g. the median and both quartiles of the satisfaction value for North America is noticeably above that of Eastern Europe). 

##### Removal of Unusual Values

We previously cleaned the dataset with respect to missing values and obvious errors.  However, when considering the context of some of our variables, some of the entries are still unusual.  Let's look at 2 plots that illustrate this

```{r, fig.height=2.5, message=FALSE, warning=FALSE}
plot1 <- teacher_data %>% ggplot(aes(x=total_time)) + geom_histogram() +
  labs(title="Histogram of Time Spent on
       Teaching Activities per Week",
       x="Time Spent on Teaching Activities (hrs/week)",
       y="Count")

plot2 <- teacher_data %>% ggplot(aes(x=age, y=years_teacher)) + geom_point(alpha=0.5,shape=".") + geom_abline(intercept =0, slope=1, color="red") +  geom_abline(intercept = -15, slope=1, color="yellow")+  geom_jitter(shape=".") +
  labs(title="Reported Age vs Years Teaching",
       x="Age (years)",
       y="Years Teaching (years)")

grid.arrange(plot1, plot2, ncol=2)
```

We can see from the histogram on the left, that most teachers seem to have a 40 hour work week, which is a pretty typical time comittment.  There is another dropoff after 60 hours/week, which is a reasonable addition to the 40 hour workweek for teachers that spend a fair amount of time class planning and grading in the evenings.  Some teachers, however, are claiming to have spent over 90 hours per week on teaching related activities.  If a teacher worked from 6 am to 9 pm Monday to Friday (15 hours a day) from time at school and grading in the evening, and another 15 hours of related teaching activities on the weekend such as class planning and grading, they would be at 90 hours in a week.  Although this is physically possible, it seems highly unlikely except for in unusual situations like boarding school.  Let's see how many teachers reported 90 hours a week or more.

```{r, echo=TRUE}
nrow(teacher_data[teacher_data$total_time==90 & !is.na(teacher_data$total_time),])
nrow(teacher_data[teacher_data$total_time>90 & !is.na(teacher_data$total_time),])
```

We can see that there are many more values of exactly 90 than greater than 90. So we keep the values of 90, but set all values that are higher than 90 hours a week to NA.

```{r, echo=TRUE}
teacher_data[teacher_data$total_time > 90 & !is.na(teacher_data$total_time), "total_time"] <- NA
```

Now, looking at the plot on the right, we can see that there are points above the red line, which are teachers who reported they had been teaching longer than they had been alive. Indeed, let's see how many responses reported years that would indicate that they had started teaching before age 15 (points above the yellow line), and set those values to NA.

```{r, echo=TRUE}
age_start <- teacher_data$age - teacher_data$years_teacher
nrow(teacher_data[age_start < 15 & !is.na(age_start),])
teacher_data[age_start < 15 & !is.na(age_start), "years_teacher"] <- NA
```

### Analysis for Research Question 1 - Linear Regression

To address our first research question, and to try to identify which variables are the best indicators of teacher job satisfaction, we are going to create a linear model.

Although our earlier plot of satisfaction against the degree of collaboration at the school shows a clear linear trend, let's look several more plots of the data against our response variable, before diving into any linear regression models:

```{r, fig.height=2, fig.width=9, message=FALSE, warning=FALSE}
plot1 <- teacher_data %>% filter(!is.na(teacher_data$train_stat)) %>% ggplot(aes(x=train_stat, y=sat_avg)) + geom_boxplot() + labs(title="Teacher Training Status vs Teacher Satisfaction", y="Teacher Satisfaction (low to high)")
plot2 <- teacher_data %>% ggplot(aes(x=years_teacher, y=sat_avg)) + geom_point(alpha=0.05) +
  labs(title="Years Spent as a Teacher
       vs Teacher Satisfaction",
       x="Years Spent as a Teacher", y="")
plot3 <- teacher_data %>% filter(!is.na(prep_A)) %>% ggplot(aes(x=prep_A, y=sat_avg)) + geom_boxplot() + labs(title="Preparation vs Satisfaction", x="Prepared for the Content I Teach", y="")

#plot4 <- teacher_data %>% ggplot(aes(x=prep_C, y=sat_avg)) + geom_boxplot() +
#  labs(title="Sense of Preparation for Classroom
#       Practice vs Teacher Satisfaction",
#       x="Degree of Preparedness for Classroom Practice (self reported)",
#       y="Teacher Satisfaction (low to high)")
grid.arrange(plot1, plot2, plot3, ncol=3)
```


```{r}
# In order to have easier control over which predictors to use in our model, below we create several lists of indices that we might want to include or exclude as a group

# We don't want to use the 10 satisfaction questions to predict sat_avg, since they were used to create the sat_avg predictor, so let's create a list of the indices we need to avoid
satcols <- grep("sat", colnames(teacher_data))
sat_indices <- satcols[satcols != which(colnames(teacher_data) == "sat_avg")]

subject_names <- c("R_WR_Lit", "Math", "Science", "Soc_Studies", "Modern_Lang", "Ancient_Lang", "Tech", "Arts", "PE", "Rel_Ethics", "Practical", "Other")
subject_indices <- c(which(colnames(teacher_data) %in% subject_names))

# We might not want to include the three questions about how prepared they felt:
prep_indices <- grep("prep_", names(teacher_data))

# Some columns also have many more NA values, lets make a list of the columns that have NA values in more than 10% of the data (this ends up being num_students and feedback_salary), that way we can choose whether or not to include them in our linear models
na_cols <- c()
for(i in 1:ncol(teacher_data)) {
  if(sum(is.na(teacher_data[,i]))/nrow(teacher_data) > 0.10) {
    na_cols <- c(na_cols,i)
  }
}
```

Dataset for Linear Regression Model: To start, for our linear regression model, we are going to remove columns with a large number of NAs, the subject data columns (we're only interested in the number of subjects each teacher is teaching, not the specific subjects), and the 10 satisfaction related questions, as they were used to create our response variable.

```{r}
x <- na.omit(teacher_data[, -c(sat_indices, na_cols, subject_indices)])
```

One of the things we are interested in is how geography impacts teacher satisfaction, however we have two different geography variables.  We can't include both in the model, as they are linearly dependent, and R will only find a set of coefficients for one of the two variables if we put both in the model.  Because of this, let's try both and compare the results:

Linear model using country (summary has been left out for space):

```{r, echo=TRUE}
m_country <- lm(sat_avg ~ ., data=x[,-which(names(x) %in% c("region"))])
summary(m_country)$r.squared
```

Linear model using region (summary has been left out for space):

```{r, echo=TRUE}
m_region <- lm(sat_avg ~ ., data=x[,-which(names(x) %in% c("country"))])
summary(m_region)$r.squared
```

If one looks at the two summaries (Summaries 1 and 2 in the Appendix), one can see that a large amount of the country categories have coefficients with significant p-values in the first model, and all of the region categories have coefficients with significant p-values in the second model.  However, the R-squared value for the model using country data was 0.2449, and the R-squared value for the model with the region predictor was 0.1983.  This means there was drop in R-squared by roughly 19%, which is quite a bit.  As a result, we are going to use the country data in the linear model. We can also see that train_stat was not significant in either of the models, so let's remove that from future models.

Lastly, it seems very possible that age and years_teacher are correlated, since teachers that have been teaching for longer are likely to be older.  As a result, we probably don't need both predictors in our linear model.  Let's check the correlation:

```{r}
cor(teacher_data$age, teacher_data$years_teacher, use = "complete.obs")
```

Age and years_teacher are pretty highly correlated (correlation of 0.846). As a result, let's see if there's another way we can use our age and years_teacher data, and combine it into one metric. One option is the ratio of years_teacher to age. It's possible that the fraction of your life that you have spent teaching, affects how satisfied you are.    

```{r}
teacher_data$age_start <- age_start
teacher_data$age_ratio <- teacher_data$years_teacher/teacher_data$age
```

Let's see whether these new predictors, or one of our older predictors, seems to have the clearest linear relationship with our response variable, sat_avg:

```{r, fig.height=2, fig.width=8, message=FALSE, warning=FALSE}
plot1 <- teacher_data %>% ggplot(aes(x=age, y=sat_avg)) + geom_point(alpha=0.05) +
  labs(title="Age vs Teacher Satisfaction", x="Age (years)", y="Teacher Satisfaction (low to high)") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6),
        plot.title=element_text(size=8))
plot2 <- teacher_data %>% ggplot(aes(x=years_teacher, y=sat_avg)) + geom_point(alpha=0.05) +
  labs(title="Years Spent as a Teacher
       vs Teacher Satisfaction",
       x="Years Spent as a Teacher",
       y="Teacher Satisfaction (low to high)") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6),
        plot.title=element_text(size=8))
plot3 <- teacher_data %>% ggplot(aes(x=age_ratio, y=sat_avg)) + geom_point(alpha=0.05) +
  labs(title="Fraction of Life Spent Teaching
       vs Teacher Satisfaction",
       x="Fraction of Life Spent Teaching",
       y="Teacher Satisfaction (low to high)") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6),
        plot.title=element_text(size=8))
grid.arrange(plot1, plot2, plot3, ncol=3)
```

From the plots above, it looks like years_teacher has the clearest linear relationship with sat_avg, so we're going to use that predictor in our linear model.

Let's also make sure that the prep_A, prep_B, and prep_C predictors are not too highly correlated.

```{r}
data <- teacher_data[, grep("prep_", names(teacher_data))]
data <- data.frame(prep_A = as.numeric(teacher_data$prep_A),
                   prep_B = as.numeric(teacher_data$prep_B),
                   prep_C = as.numeric(teacher_data$prep_C))
cor(data, use = "complete.obs")
```

We can see from the correlation matrix above that all three prep questions are pretty correlated, and therefore we shouldn't put all three in our linear model.  Let's try creating a single predictor that's the average of all three, in order to indicate teachers' overall sense of preparedness.

```{r}
teacher_data$avg_prep <- apply(data, 1, mean)
```

Now let's make a new dataset with all of the variable selections we just made. So we need to remove region, train_stat, age, age_start, age_ratio, and all three prep predictors.

```{r, echo=TRUE}
x.new <- teacher_data[, -c(sat_indices, na_cols, subject_indices, prep_indices)]
x.new <- na.omit(x.new[, -which(names(x.new) %in% c("train_stat", "region", "age", "age_start", "age_ratio"))])
```

Now let's see if all of the variables that we've decided to use for the linear model, are normally distributed, or if some transformations might be helpful:We can see from the histogram of sat_avg (Plot 1, displayed in the Dataset section), that although it isn't perfectly normal, it is close enough that it probably doesn't warrant any transformation of the variable. 

```{r, fig.height=2, fig.width=8, message=FALSE, warning=FALSE}
h1 <- x.new %>% ggplot(aes(x=total_time)) + geom_histogram() +
  labs(title="Histogram of Time Spent on
       Teaching Activities",
       x="Time Spent on Teaching Related Activities",
       y="Count") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6),
        plot.title=element_text(size=8))

h2 <- x.new %>% ggplot(aes(x=years_teacher)) + geom_histogram() +
  labs(title="Histogram of Years Spent
       as a Teacher",
       x="Years of Teaching Experience",
       y="Count") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6),
        plot.title=element_text(size=8))

h3 <- x.new %>% ggplot(aes(x=collab)) + geom_bar() +
  labs(title="Bar Graph of Collaboration Rating",
       x="How Collaborative is School Culture",
       y="Count") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6),
        plot.title=element_text(size=8))

h4 <- x.new %>% ggplot(aes(x=num_subjects)) + geom_bar() +
  labs(title="Histogram of The Number
       of Subjects Taught",
       x="Number Subjects Taught",
       y="Count") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6),
        plot.title=element_text(size=8))

grid.arrange(h1, h2, h3, h4, ncol = 4)
```

Three of the plots above, although not perfectly normal, don't have a huge amount of skew.  In fact, the histogram for time spent on teaching related activities has very little skew.  As a result, we probably don't need to transform any of these variables.  The number of subjects histogram does have a lot of skew.  However, we tried several different transformations, and none of them seemed to help much.  After the creating the model, we will look at the normal Q-Q plot to make sure the normality assumption wasn't violated too significantly.

Now that we have a new data set with all of our variable selections, let's make a final model (summary removed from report for the sake of space):
```{r, echo=TRUE}
m_final <- lm(sat_avg ~ ., data=x.new)
summary(m_final)$r.squared
```

Before analyzing this model, let's make sure that the assumptions for linear regression hold.

```{r, fig.height=3}
par(mfrow=c(1,2))
plot(m_final, which=1)
plot(m_final, which=2)
```

We can see from the two plots above that there are no trends in the residuals, and the Normal Q-Q plot is close to a straight line, so our assumptions hold.

**General Analysis of the Final Linear Model:**

Looking at the model summary (Summary 3 in Appendix), our final linear model has an R-squared value of 0.24.  Many of the countries have coefficients with significant p-values, and all of the other predictors have coefficients with significant p-values at a level of 0.001.  However, this is not that surprising given how large our dataset is.  We might be able to get more insight by analyzing the values of the coefficients.

In order to decide which predictors seem to be particularly helpful in predicting teacher satisfaction, we will look at the magnitude of the coefficients in the model summary and the confidence intervals of the coefficients (Summary 4 in Appendix) and see how far away they are from zero.

```{r, include=FALSE}
confint(m_final)
```

The largest coefficients in the model in terms of magnitude were: collab.L, countryMEX, countryFIN, countryBFL, countryMYS, and countryNLD.  All the other coefficients had magnitudes that were less than 0.2.

Looking at the confidence intervals of all of the predictors other than country, we can see that the confidence interval for the coefficient of num_subjects is pretty close to zero (it has a range of 0.0017 to 0.0054).  The confidence interval for the coefficient gender is also pretty close to zero, with a range of -0.027 to -0.016.  Lastly, two of the confidence intervals closest to zero are for the coefficient of years_teacher (0.00045, 0.00095) and the coefficient for total_time (-0.00063,-0.00031).  Given the small magnitudes of the coefficients, and how close the confidence intervals of these four predictors are to zero, they do not seem that significant in predicting teacher satisfaction.

On the other hand, the coefficients for collab.L and avg_prep had confidence intervals of roughly (0.55, 0.57) and (0.11, 0.12) respectively.  Given that teacher satisfaction was on a scale of 1 to 4, a coefficient for one of the categories of the collab variable with a magnitude of around 0.5 seems pretty significant. A coefficient of a little over 0.1 also seems significant relative to how small many of the other coefficients were.

Moreover, we can see that several of the countries have coefficients with pretty large magnitudes, and confidence intervals for those coefficients that are pretty far from zero.  For example, the confidence interval for the coefficient for countryMEX is (0.4494836350, 0.4990107641) and for countryFIN is (0.2306808115, 0.2792257572). In fact, 15 countries have coefficients with a magnitude of at least 0.1, and a confidence interval for that coefficient that is also at least 0.1 away from 0.  This suggests that country is also significant in predicting teacher satisfaction.

Lastly, let's look at the sign of the coefficients in the model, to see the direction of the relationships between the predictors and sat_avg.  What is positively correlated with sat_avg? What is negatively correlated with sat_avg?  Do these directions make sense?:

Of course we should keep in mind the fact that there are so many factors in this model and thus the coefficients may not completely explain the relationship of the variables.

years_teacher has a very small positive coefficient (0.0006979) with sat_avg. A positive correlation may make sense, as those who are satisfied with being a teacher are more likely to stay for longer. However, this coefficient is very small. total_time has a very small negative coefficient (-0.0004706). The size of this may be because of the duality between those who are overworked being unsatisfied, while also addressing that those who are satisfied with their jobs may be more inclined to work more. From this coefficient alone it seems that the first phenomenon may be more prominant. The collab coefficients actually get larger and larger with higher levels of belief that the work environment is collaborative. This makes a lot of sense, as a more collaborative work environment is generally thought of as a positive aspect and thus may be positively correlated with satisfaction. It seems that there is also a positive coefficient for the number of subjects taught and a negative coefficient for gender being male, which do not have any obvious intuition either for or against, but are interesting to see. One potential explanation for number of subjects taught could be that a satisfied teacher is willing to put in more work and teach more than one subject.  Finally, there is a positive coefficient for the average amount of preparation, which may make sense as those who are satisfied may be more inclined to prepare more and those who feel prepared may be more satisfied.

**Conclusion:** The results of this linear model suggest that teachers perceptions of how collaborative the school culture is, as well as how prepared the teachers feel, are significant predictors for how satisfied teachers are.  Interestingly, although how prepared teachers felt was significant in predicting teacher satisfaction, the degree of teacher training was not.  It is possible that this is because it was a Yes or No question on the survey, and therefore we didn't have more information on the quantity or quality of training that teachers received, whereas teachers were able to rate how prepared they felt in several areas on a scale of "Not at all", "Somewhat", "Well", and "Very well".  Lastly, country was very useful in predicting teacher satisfaction.  This could be due to a variety of reasons, including cultural values and attitudes at work, school legislation and restrictions, funding of schools, the ratio of public to private schools, and the respect that the teaching profession receives, all of which vary between countries, and are environmental factors at the country level, rather than the kind of information available in a teacher survey.

### Analysis for Research Question 2

Given how useful country was in predicting teacher_satisfaction, it would be interesting to see whether there are any significant regional differences in some of the other significant predictors from our linear model above. For example, in the regions with higher teacher satisfaction, are collaboration ratings also higher? Therefore, let's now address our second research question, and try to identify the differences across different regions for teacher job satisfaction and other classroom factors. First, lets look at a map of average teacher satisfaction by country.

```{r, warning=FALSE, include=FALSE}
# set to ISO codes
teacher_data$country <- revalue(teacher_data$country, c("AAD"="ARE", "BFL"="BEL","CAB"="CAN","CSH"="CHN","ENG"="GBR"))
# apparently plyr dependencies will mess things up
detach(package:plyr)
country_avg <- teacher_data %>% group_by(country) %>% summarize(mean_satisfaction = mean(sat_avg, na.rm=TRUE))
library(plyr); library(dplyr)
#code for map with average teacher satisfaction by country
sPDF1 <- joinCountryData2Map(country_avg, joinCode = "ISO3", nameJoinColumn = "country")
```

```{r, fig.height=2.5, fig.width=9, message=FALSE, warning=FALSE}
par(mar=c(0.1, 0.1, 0.1, 0.1))
mapParams1 <- mapCountryData(sPDF1, nameColumnToPlot = "mean_satisfaction", colourPalette="terrain", oceanCol = "lightblue", missingCountryCol="gray", addLegend = FALSE)
do.call( addMapLegend, c(mapParams1, legendWidth=0.5, legendShrink=0.5))
```

Below is also an anova test showing the significance of the difference in satisfaction by region.

```{r}
# Regression of sat_avg vs region
msatreg <- lm(sat_avg ~ country, data=teacher_data)

# ANOVA of regression
anova(msatreg)
```

When we first began analyzing factors by country, we grouped the countries into 8 regions using the World Health Organization geographical categories. The difference in mean satisfaction was still statistically significant (p-value < 0.05) across all region pairs other than Western Europe vs Eastern Europe, as shown in the pairwise.t.test in the appendix.

However, we felt that geographical groups didn't capture all the factors that might influence teacher satisfaction by country (for instance, China and Australia were grouped together geographically, but have vastly different governments). Thus, we took the 2 most satisfied countries (Mexico and Malaysia), 2 least satisfied countries (Slovak Republic and Japan) , and two countries in the middle of the list (Serbia and Romania), and decided to analyze that subset of countries.

```{r}
country_avg[order(country_avg$mean_satisfaction)[c(1,2,18,19,35,36)],]
td_country <-teacher_data[teacher_data$country %in% c("SVK", "JPN", "SRB", "ROU", "MYS", "MEX"),]
# remove unused factors
td_country$country <- factor(td_country$country, levels=c("SVK", "JPN",  "SRB", "ROU", "MYS", "MEX"))
```

Based on the predictors affected job satisfaction in Part 1, we can see which countries stand out, either positively or negatively, in those factors.

```{r, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
# collab plot
p41 <- td_country %>% filter(!is.na(collab)) %>% ggplot(aes(x = product(collab, country), fill=factor(collab)), na.rm=TRUE) +  geom_mosaic() + scale_fill_manual(values=c("red3","indianred1", "lightgreen", "green3")) +  theme(axis.text.x=element_text(angle=-25, hjust= .1)) + labs(x="Country", title='There is a collaboratie school culture.') + guides(fill=guide_legend(title = "Response", reverse = TRUE)) + theme(axis.text=element_text(size=6), axis.title=element_text(size=6), plot.title=element_text(size=8))

p42 <- td_country %>% filter(!is.na(prep_A)) %>% ggplot(aes(x = product(prep_A, country), fill=factor(prep_A)), na.rm=TRUE) +  geom_mosaic() + scale_fill_manual(values=c("red3","indianred1", "lightgreen", "green3")) +  theme(axis.text.x=element_text(angle=-25, hjust= .1)) + labs(x="Country", title='How prepared do you feel for the content of the subjects you teach?') + guides(fill=guide_legend(title = "Response", reverse = TRUE)) + theme(axis.text=element_text(size=6), axis.title=element_text(size=6), plot.title=element_text(size=8))

grid.arrange(p41,p42, ncol=2)
```

Indeed, from an anova we can see that collab and prep_A are themselves significant in variation by country.

```{r}
# Check which variables have a significant correlation with region
anova(lm(as.numeric(collab) ~ country, data=teacher_data))
anova(lm(as.numeric(avg_prep) ~ country, data=teacher_data))
```

Looking at the first plot, there isn't really any trend, but Mexico jumps out as an outlier. It's the country with the highest teacher satisfaction, but almost 30% of teachers say their school isn't collaborative. Does this mean that collaboration doesn't matter as much to satisfaction in Mexico? We c

```{r, fig.height=1}
td_country %>%
    filter(country == "MEX") %>% filter(!is.na(collab)) %>%
    ggplot(aes(x=collab, y=sat_avg, col=country)) + geom_boxplot(varwidth=TRUE) + labs(title="School Collaboration vs Satisfaction in Mexico")

```

Teacher satisfaction is still correlated positively with teacher satisfaction in Mexico, so there must be some other factor that makes the average satisfaction in Mexico higher.

Mexico is also an outlier in the second plot, where 30% of teachers did not feel prepared to teach their subject well.

The takeaway is that although there are predictors that correlate positively with higher teacher satisfaction, like collaborative schools, and countries that have significantly higher teacher satisfaction than others, like Mexico, the countries with more satisfied teachers don't necessarily have the most collaborative schools. Teacher satisfaction is complex, and we have explored a very small part of the picture.

# Conclusion:

Overall, our models didn't completely explain the variation in the data. We expected real datasets to be messier than the ones we had dealt with in problem sets, and so we were pleased with the observations we did find.

The first thing we were worried about in this project was how satisfaction was related to the different factors, and as mentioned previously it seems that the most significant factors by linear model coefficient were collab.L and avg_prep. Moreover, we noted that many of the country coefficients were very large, implying a large amount of variation between the satisfaction averages across countries.

This brings us to our second research question, which addressed how various factors changed across countries. We first confirmed the above finding with a graph of satisfaction for each country, and supported this with an anova. Then, we also noted that some of the factors that were important to satisfaction, in particular level of collaboration and amount of preparation were themselves also significantly different across countries. And in fact, in addition to noting that these factors themselves varied across countries, we showed that the relationship of these factors (in particular the relationship of preparation level to satisfaction) also varied by country.

Before we started looking into the data, we expected there to be a lot more unsatisfied teachers than there actually were. Indeed, if we take `sat_avg` scores of above 2.5 to mean that the respondent is generally satisfied (if on average, they responded Agree or Strongly Agree more than Disagree or Strongly Disgree on the teacher satisfaction questions), then

```{r, echo=TRUE}
nrow(teacher_data[teacher_data$sat_avg >=2.5,])/nrow(teacher_data)
```

of our respondents were satisfied. Of course, this is a very low baseline (it just means the teacher didn't respond "Agree" or "Strongly Agree" to a question like "I regret that I decided to become a teacher.") If we were to collect the data particularly to understand how various factors affect teacher satisfication, we would want to get a more precise sense of how satisfied or unsatisfied teachers are about specific aspects of teaching. We might ask respondents to our survey to rate, on a scale from 1 - 10, their satisfaction with things like working conditions, pay, sense of impact, and more. 

As another problem, we don’t know how many teachers were sampled comparatively by country - we know it was no more than 22 per school, but were unable to analyze how representative our sample was of a country as a whole. In addition, there were some country codes that were just cities (e.g. all our china teachers came from Shanghai).

As far as concerns about the data, when we did our data cleaning, we did find some suspicious responses. There were things we could say were impossible, such as working longer than one has been alive, but we could have other incorrectly reported responses that we weren't  able to immediately mark as suspicious. Because all of the data was a self reported survey, inaccurate/falsely reported answers were pretty inevitable, but are still a concern about the data.

One area that our analysis would have benefited from other data sources would be countries. We created our `regions` variable using the World Health Organization regions categories, so that it was easier to visualize and interpret our results by group of countries. However, there are many other ways we could have tried to categorize the countries - for instance, because more collaborative school environments correlated with higher teacher satisfaction, we could have tried to group the countries by their rankings on the individualism vs collectivism scale, to see whether national attitudes made a satistically significant difference.

### Code Appendix
Code Outputs

```{r}
pairwise.t.test(teacher_data$sat_avg, teacher_data$region, p.adj="bonf")
```

##### Summary 1
```{r}
summary(m_country)
```

##### Summary 2
```{r}
summary(m_region)
```

##### Summary 3
```{r}
summary(m_final)
```

##### Summary 4
```{r}
confint(m_final)
```

Data Cleaning Code from Data Exploration.Rmd
```{r data_cleaning,eval=FALSE}
# Lower-secondary-school teacher data (international file, all countries) is the file titled BTGINTT2
lower_second_data <- read.spss("SPSS_International/BTGINTT2.sav", to.data.frame=TRUE, use.missings=TRUE)

cols <- c("CNTRY", "TT2G01", "TT2G02", "TT2G05B", "TT2G11", "TT2G13A", "TT2G13B", "TT2G13C", "TT2G16", "TT2G30G", "TT2G38", "TT2G44E", "TJOBSATS")

# We are creating a dataset with the columns from our project proposal, the 10 questions related to job satisfaction, which are columns TT2G46A through TT2G46J, as well as a few additional columns which we also think might be useful
data <- cbind(lower_second_data[,cols],lower_second_data[,grep("TT2G46", colnames(lower_second_data))], lower_second_data[,grep("TT2G15", colnames(lower_second_data))])

# Rename some of the columns so that they're easier to work with
data <- data %>% rename(gender=TT2G01, age=TT2G02, country=CNTRY, years_teacher = TT2G05B, train_stat = TT2G11, prep_A=TT2G13A, prep_B=TT2G13B, prep_C=TT2G13C, total_time=TT2G16, feedback_salary=TT2G30G, collab=TT2G44E, num_students=TT2G38, satisfaction=TJOBSATS)

# Columns that start with TT2G15 correspond to several questions asking what subjects teachers teach:
#Let's manipulate the subject data so that it's in a form that's useful to us

# First make an ID column to merge back after isolating the subject as one column
data$ID <- seq.int(nrow(data))

# Indices of columns dealing with subject taught
indices <- grep("TT2G15", colnames(data))[-1] #-1 to deal with the TT2G15 column that isn't for a specific subject

# Convert "Yes" "No" levels to 0 for didn't teach subject and 1 for taught subject
for (i in indices) {
  data[,i] <- 2 - as.numeric(data[,i])
}

# Rename columns by subject
data <- data %>% rename(R_WR_Lit=TT2G15A, Math=TT2G15B, Science=TT2G15C, Soc_Studies = TT2G15D, For_Lang = TT2G15E, Gk_Ln=TT2G15F, Tech=TT2G15G, Arts=TT2G15H, PE=TT2G15I, Rel_Ethics=TT2G15J, Practical=TT2G15K, Other=TT2G15L)

# Names of columns 
col_names <- colnames(data)[indices]

# Below is just the R code for extracting the subject column
# This is separated for clarity since it is extensive in and of itself

df <- data[25:37]

# Melt to format -- similar to "gather"
temp_by_subj <- melt(df,id="ID")

# Only keep 1's (subjects taught)
temp_by_subj <- temp_by_subj[which(temp_by_subj$value==1),]

# Merge the subject table in with data
data_by_subject <- merge(data,temp_by_subj,all.y=T, by = "ID")

# Clean up unnecessary columns
data_by_subject <- data_by_subject[,-(c(15:24,26:37,39))]
data_by_subject <- data_by_subject %>% rename(Subject=variable)

# Below is the code used to clean up the satisfaction related questions, as well as a few other columns

# All ten columns with responses related to job satisfaction are factors, with 4 levels: "Strongly Disagree", "Disagree", "Agree", "Strongly Agree".
# We are going to convert these 10 columns to numerical data, where the factors get converted to the integers 1, 2, 3, and 4, with 1 corresponding to Strongly Disagree and 4 corresponding to Strongly Agree.
for (i in grep("TT2G46", colnames(data))) {
  data[,i] <- as.numeric(data[,i])
}

# Questions C, D, and F were negatively phrased questions, so we are reversing their scales so we can compare them to the positively phrased questions.
data$TT2G46C <- 5 - data$TT2G46C
data$TT2G46D <- 5 - data$TT2G46D
data$TT2G46F <- 5 - data$TT2G46F

# Questions TT2G46A through J were all related to teacher satisfaction.  Consequently, we are going to create a column that averages the results of these 10 questions to get a sense of teacher satisfaction
data$TT2G46_avg <- rowMeans(data[,grep("TT2G46", colnames(data))], na.rm=TRUE)

# Clean up country column, which has a lot of whitespace
data$country <- as.factor(trimws(as.character(data$country)))

# Clean up train_stat variable so it's easier to work with
# Make No the first factor, so that if we convert it to numeric, it's easy to have 0 represent No and 1 represent Yes
data$train_stat <- relevel(data$train_stat, "No")
```


*TODO: note, we need to figure out how many code chunks in final, and then remove the last one here, because otherwise the datacleaning code will be printed twice 
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```
