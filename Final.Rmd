---
title: "STAT 230 Final Project"
author: "Veena Advani, Annie Chen, Robert Tung"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library("foreign")
library("tidyr")
library("countrycode")
library("rworldmap")
library(ggplot2)
library(reshape2)
library(grid)
library(gridExtra)
library(leaps)
library(GGally)
library(ggmosaic)
library(plyr)
library("dplyr")
```

```{r, include=FALSE}
# To start, let's read in the cleaned datasets from our last submission:
teacher_data <- read.csv("TALIS.csv")

# Change column names of 10 satisfaction questions to make more readable
satcols <- grep("TT2G46", colnames(teacher_data))
indices <- satcols[satcols != which(colnames(teacher_data) == "TT2G46_avg")]
colnames(teacher_data)[indices] <- c("sat1", "sat2", "sat3", "sat4", "sat5", "sat6", "sat7", "sat8", "sat9", "sat10")
names(teacher_data)[names(teacher_data)=="TT2G46_avg"] <- "sat_avg"

# Column "TT2G15" contains no useful data, since it's all NAs, so we're going to remove it
# Column "ID" is no longer necessary, was used for making the subject dataset
teacher_data <- teacher_data[,-which(names(teacher_data) %in% c("TT2G15","ID"))]

# Change some of the subject names (one of the comments in our dataset submission was that they weren't clear):
names(teacher_data)[names(teacher_data) == "For_Lang"] <- "Modern_Lang"
names(teacher_data)[names(teacher_data) == "Gk_Ln"] <- "Ancient_Lang"

# We also thought it might be useful to have a variable that indicates the number of subjects a given teacher teaches, so let's add it below:
subject_names <- c("R_WR_Lit", "Math", "Science", "Soc_Studies", "Modern_Lang", "Ancient_Lang", "Tech", "Arts", "PE", "Rel_Ethics", "Practical", "Other")
subject_indices <- c(which(colnames(teacher_data) %in% subject_names))

teacher_data$num_subjects <- rowSums(teacher_data[,subject_indices], na.rm=TRUE)

# We discussed in our dataset submission why we weren't going to use their teacher satisfaction variable, and we are instead going to use the average of the 10 satisfaction questions, as a result, let's remove the teacher satisfaction variable:
teacher_data <- teacher_data[, -which(names(teacher_data) %in% c("satisfaction"))]

# A lot of the variables stored as categorical variables with the categories "Strongly Disagree", "Disagree", "Strongly Agree", and "Agree", are currently factors.  Given that these categories do have a natural ordering, we're going to convert them into ordered factors.
teacher_data$prep_A <- factor(teacher_data$prep_A, levels=c("Not at all", "Somewhat", "Well", "Very well"), ordered=TRUE)
teacher_data$prep_B <- factor(teacher_data$prep_B, levels=c("Not at all", "Somewhat", "Well", "Very well"), ordered=TRUE)
teacher_data$prep_C <- factor(teacher_data$prep_C, levels=c("Not at all", "Somewhat", "Well", "Very well"), ordered=TRUE)
teacher_data$feedback_salary <- factor(teacher_data$feedback_salary, levels=c("No positive change", "A small change", "A moderate change", "A large change"), ordered=TRUE)
teacher_data$collab <- factor(teacher_data$collab, levels=c("Strongly disagree", "Disagree", "Agree", "Strongly agree"), ordered=TRUE)

# We'll split the country data into regions using the World Health Organization  categories, so that we can try to find regional patterns

# Brazil, Chile
teacher_data$region[teacher_data$country %in% c("BRA", "CHL")] <- "South America"

# Canada, Mexico, United States
teacher_data$region[teacher_data$country %in% c("CAB", "MEX", "USA")] <- "North America"

#  Bulgaria, Czech Republic,  Poland,  Romania, Russia, Slovak Republic
teacher_data$region[teacher_data$country %in% 
                        c("BGR", "CZE", "POL", "ROU", "RUS", "SVK")] <- "Eastern Europe"

# Denmark, England, Estonia, Finland,   Latvia,  Norway, Sweden
teacher_data$region[teacher_data$country %in% 
                        c("DNK", "ENG",  "EST", "FIN", "LVA", "NOR", "SWE")] <- "Northern Europe"

# Spain, Croatia, Italy, Latvia,  Portugal,  Serbia, 
teacher_data$region[teacher_data$country %in% 
                        c("ESP", "HRV", "ITA", "LVA", "PRT", "SRB")] <- "Southern Europe"

# Belgium,  France, Netherlands
teacher_data$region[teacher_data$country %in% 
                        c("BFL", "FRA","NLD")] <- "Western Europe"

# Abu Dhabi, Georgia, Israel                     
teacher_data$region[teacher_data$country %in% c("AAD", "GEO", "ISR")] <- "East Mediterranean"

# Australia, China, Japan, South Korea, Malaysia, New Zealand, Singapore
teacher_data$region[teacher_data$country %in% c("AUS", "CSH", "JPN", "KOR", "MYS", "NZL", "SGP")] <- "Western Pacific"

teacher_data$region <- as.factor(teacher_data$region)
```

### Introduction
  This analysis will focus on the Teaching and Learning International Survey (TALIS) Dataset. This data set was coordinated by the Organization for Economic Cooperation and Development (OECD), an intergovernmental organization of industrialized countries. The TALIS dataset comes from a survey of the teaching workforce that aims to collect information about teaching as a profession, the working conditions of teachers, and the learning environments of schools. Another goal of the TALIS survey was to provide a more global view of education systems, and to produce metrics that could be used to compare education systems from different countries. TALIS was administered in 2008 and 2013. We will be using the more recent international dataset, from 2013. It contains answers from teachers from more than 10,000 schools in 36 different countries. Not all of the countries were English speaking. The only eligibility requirement was that countries must be industrialized. The TALIS dataset contains several different files, which are split by school type. Although the dataset contains information from a variety of school types (primary, secondary, etc), the focus of the survey was on lower-secondary schools, and that is the largest data file. As a result, we are going to focus our analysis on lower-secondary schools. At each school that was selected to participate, the principal (or head administrator) and a random selection of up to 22 teachers were chosen to complete voluntary online questionnaires. Our analysis below consists of only the teacher responses, not the responses of the principals, as they were given slightly different questionnaires.
  
  (1) Our 1st question is: Which variables and classroom factors are the best predictors of teacher job satisfaction?
  
  There are several reasons motivating this question. Education is incredibly important to any country's development and functioning. However, there is a fair amount of diversity across countries with regard to how well the teaching profession is respected, and how well education systems are funded.  As a result, teacher working conditions vary quite a bit. In order to make sure that there are enough teachers, and that they are able to teach their students well, it's important to make sure that teacher satisfaction is at a high enough level. If teacher satisfaction is too low, it becomes difficult to retain a large enough teaching force, and to attract talented people to the field. Low teacher satisfaction can also indicate that they are having trouble doing their job well, possibly due to environmental factors out of their control. For many reasons such as these, teacher satisfaction is a useful metric to observe in an education system.  It's also helpful to understand what kinds of environments keep teachers more satisfied, so that educational systems can encourage and build these types of environments. Therefore, understanding how teacher satisfaction varies across the 36 countries in the data set, and which aspects of a school and environment correlate most with teacher satisfaction, could be helpful information.
  
  (2) Our 2nd question is: How does teacher satisfaction, and some of the significant predictors found from question 1, vary by country?

Given that one of the reasons the TALIS dataset was created was to enable cross country comparison, we thought it would be interesting to look at how some of the variables in the dataset varied by country. This question is also motivated by the fact that looking at how teacher satisfaction varies by region could be particularly helpful to educators and policy makers. There are a lot of environmental factors that can affect teacher satisfaction, and that are not addressed in the TALIS questionnaire.  However, if we can identify which countries have particularly satisfied teachers, then perhaps people who are looking to increase teacher satisfaction can learn something by further researching how schools work in those countries, past the information covered in the questionnaire. Additionally, if we are able to identify any variables that are particularly helpful in predicting teacher satisfaction (from research question 1), it would be interesting to see if the countries that have a higher average teacher satisfaction, also have values for those significant variables that were predictive of high satisfaction. Or in other words, after identifying qualities that seem to correlate with high teacher satisfaction, do the more satisfied countries seem to have those qualitites? Or does it seem like other factors, that are possibly not in the data set, are contributing to the higher levels of teacher satisfaction in those countries?

### Data

Each row of the dataset corresponds to single teacher's response to the questionnaire (There are 117876 rows).  Each column in the dataset corresponds to either a particular question in the questionnaire, or a summary variable for a section of the questionnaire.

The dataset contains a column for each individual question on the questionnaire, and summary columns for overall sections. For example, there are individual columns for questions such as “The advantages of this profession clearly outweigh the disadvantages” and “I would recommend my school as a good place to work”, but also an overall column summing up all the scores for the teacher satisfaction questions. There are columns concerning employment status, personal information/background, time spent, professional development, efficacy and amount of feedback, outlook about the school, school climate, overall job satisfaction, and much more. The specific variables we are using are listed below. Almost all of the questions in the questionnaire were multiple choice or numerical input questions.

The variables we are using are:

* `country (CNTRY):` Country of teacher (factor variable, 36 levels)
* `region:` Region the teacher's country is in (factor variable, 8 levels)
* `gender (TT2G01):` Teacher Gender (factor variable, 2 levels)
* `age (TT2G02):` Teacher Age (numeric variable)
* `years_teacher (TT2G05B):` Years of teaching experience (numeric variable)
* `train_stat (TT2G11):` Training level of teacher (factor variable)
* `prep_A/B/C (TT2G13A/B/C):` How prepared the teacher felt with respect to 3 different areas of teaching, which were content, pedagogy and classroom practice respectively (ordered factor variable, 4 levels)
* `avg_prep:` The average of each teachers responses for prep_A/B/C (numeric variable between 1 and 4)
* `total_time (TT2G16):` Total time spent on teaching related activities in hours per week (numeric variable)
* `feedback_salary (TT2G30G):` Whether performance feedback influences salary (ordered factor variable, 4 levels)
* `collab (TT2G44E):` How collaborative is the school culture? (ordered factor variable, 4 levels)
* `num_students (TT2G38):` Number of students in the teacher’s class (numeric variable)
* `satisfaction (TJOBSATS):` Self rated teacher job satisfaction (ordered factor variable, 4 levels)
* `Various Subject Names (TT2G15[A - L]):` 12 columns representing which subjects the teacher teaches (boolean numeric variable)
* `num_subjects:` The number of subjects the teacher teaches (numeric variable)
* `sat1-sat10 (TT2G46[A - J]):` Ten questions related to teacher job satisfaction (ordered factor variables, 4 levels)
* `sat_avg:` An average of the ten questions related to teacher job satisfaction (numeric variable)

##### Initial Plots Describing Data

Below are three plots that show our data and hopefully motivate our research questions and the analysis.

```{r, warning=FALSE, echo=FALSE}
# Histogram of teacher satisfaction
p1 <- teacher_data %>% ggplot(aes(x=sat_avg)) + geom_histogram(binwidth = 0.2) + labs(title="Plot 1: Histogram of Teacher Satisfaction", x="Teacher Satisfaction (low to high)", y="Count")

# Boxplot of whether they consider their school a collaborative environment to satisfaction
p2 <- teacher_data %>% filter(!is.na(collab)) %>% ggplot(aes(x=collab, y=sat_avg)) + geom_boxplot() +
  labs(title="Plot 2: Relationship Between Collaboration Level
       of the School and Teacher Satisfaction",
       x="Response to 'There is a collaborative school culture
       which is characterised by mutual support'",
       y="Teacher Satisfaction")

# Boxplot of region to satisfaction
p3 <- teacher_data %>% filter(!is.na(sat_avg)) %>% ggplot(aes(x=reorder(region, -sat_avg, median), y=sat_avg)) + geom_boxplot() + labs(title="Plot 3: Teacher Satisfaction by Region", x="Region", y="Teacher Satisfaction")
```

```{r, echo=FALSE, fig.height=2.25, fig.width=10, message=FALSE, warning=FALSE}
grid.arrange(p1, p2, ncol=2)
```

```{r, echo=FALSE, fig.height=2.25, fig.width=9}
p3
```

**Plot 1** As we are later analyzing the effect of different variables on teacher satisfaction, and the variation between different regions, it is important to note the overall distribution of our response variable. 

**Plot 2** provides a box plot of collab vs sat_avg. This plot is an example of one we would find in regard to our first research question, in which we assess which variables are particularly predictive of overall teacher satisfaction. In looking at this particular plot, we can see a strong visual trend between how collaborative a teacher viewed his/her school environment to be, and how satisfied that teacher was. We will provide similar plots for other variables in the analysis of our first research question, as well as more rigorous analysis.

**Plot 3** provides a box plot of geographic region vs sat_avg. This is relevant to our 2nd question, which revolves around variations in sat_avg and other variables across geographical regions. Visually, while all median values for sat_avg hover around the range of 2.5 to 3.5, there is still a noticeable difference in some of the distributions (e.g. the median and both quartiles of the satisfaction value for North America is noticeably above that of Eastern Europe). We will explore more plots like this one, but with other variables, in our full analysis.

##### Removal of Unusual Values

We previously cleaned the dataset with respect to missing values and obvious errors.  However, when considering the context of some of our variables, some of the entries are still unusual.  

Let's look at the variable containing total time spent on teaching related activities per week:
```{r, fig.height=2.5, message=FALSE, warning=FALSE}
teacher_data %>% ggplot(aes(x=total_time)) + geom_histogram() + labs(title="Histogram of Time Spent on Teaching Activities per Week", x="Time Spent on Teaching Activities (hrs/week)", y="Count")
```

We can see from this histogram, that most teachers seem to have a 40 hour work week, which is a pretty typical time comittment.  There is another dropoff after 60 hours/week, which is a reasonable addition to the 40 hour workweek for teachers that spend a fair amount of time class planning and grading in the evenings.  We can also see that there are some teachers claiming to have spent over 90 hours per week on teaching related activities.  If a teacher worked from 6 am to 9 pm Monday through Friday (15 hours a day) due to time at school and grading in the evening, and another 15 hours of related teaching activities on the weekend such as class planning and grading, they would be at 90 hours in a week.  Although this is physically possible, it seems highly unlikely except for in unusual situations like boarding school.  Given this fact, let's see how many teachers reported 90 hours a week or more.

```{r, echo=TRUE}
length(teacher_data[teacher_data$total_time>=90 & !is.na(teacher_data$total_time),]$total_time)
length(teacher_data[teacher_data$total_time>90 & !is.na(teacher_data$total_time),]$total_time)
```

We can see that there are many more values of exactly 90 than greater than 90. Given this fact, we will keep the values of 90, but set all values that are higher than 90 hours a week to NA.

```{r, echo=TRUE}
teacher_data[teacher_data$total_time > 90 & !is.na(teacher_data$total_time), "total_time"] <- NA
```

We can now take a look at reported age vs years teaching. 
```{r, fig.height=2.5}
teacher_data %>% ggplot(aes(x=age, y=years_teacher)) + geom_point(alpha=0.5,shape=".") + geom_abline(intercept =0, slope=1, color="red") +  geom_abline(intercept = -15, slope=1, color="yellow")+  geom_jitter(shape=".") + labs(title="Reported Age vs Years Teaching", x="Age (years)", y="Years Teaching (years)")
```

We can see that there are points above the red line, which are teachers who reported they had been teaching longer than they had been alive. Indeed, let's see how many responses reported years that would indicate that they had started teaching before age 15 (points above the yellow line), and set those values to NA.

```{r, echo=TRUE}
age_start <- teacher_data$age - teacher_data$years_teacher
nrow(teacher_data[age_start < 15 & !is.na(age_start),])
teacher_data[age_start < 15 & !is.na(age_start), "years_teacher"] <- NA
```

### Analysis for Research Question 1 - Linear Regression

To address our first research question, and to try to identify which variables are the best indicators of teacher job satisfaction, we are going to create a linear model.

Although our earlier plot of satisfaction against the degree of collaboration at the school shows a clear linear trend, let's look at a matrix of plots of the data, before diving into any linear regression models:
```{r}
#TODO
#years teacher plot?
#ggpairs(teacher_data)
```


```{r}
# In order to have easier control over which predictors to use in our model, below we create several lists of indices that we might want to include or exclude as a group

# We don't want to use the 10 satisfaction questions to predict sat_avg, since they were used to create the sat_avg predictor, so let's create a list of the indices we need to avoid
satcols <- grep("sat", colnames(teacher_data))
sat_indices <- satcols[satcols != which(colnames(teacher_data) == "sat_avg")]

subject_names <- c("R_WR_Lit", "Math", "Science", "Soc_Studies", "Modern_Lang", "Ancient_Lang", "Tech", "Arts", "PE", "Rel_Ethics", "Practical", "Other")
subject_indices <- c(which(colnames(teacher_data) %in% subject_names))

# We might not want to include the three questions about how prepared they felt:
prep_indices <- grep("prep_", names(teacher_data))

# Some columns also have many more NA values, lets make a list of the columns that have NA values in more than 10% of the data (this ends up being num_students and feedback_salary), that way we can choose whether or not to include them in our linear models
na_cols <- c()
for(i in 1:ncol(teacher_data)) {
  if(sum(is.na(teacher_data[,i]))/nrow(teacher_data) > 0.10) {
    na_cols <- c(na_cols,i)
  }
}
```

Dataset for Linear Regression Model:
To start, for our linear regression model, we are going to remove columns with a large number of NAs, the subject data columns (we're only interested in the number of subjects each teacher is teaching, not the specific subjects), and the 10 satisfaction related questions, as they were used to create our response variable.
```{r}
x <- na.omit(teacher_data[, -c(sat_indices, na_cols, subject_indices)])
```

One of the things we are interested in is how geography impacts teacher satisfaction, however we have two different geography variables.  We can't include both in the model, as they are linearly dependent, and R will only find a set of coefficients for one of the two variables if we put both in the model.  Because of this, let's try both and compare the results:

Linear model using country (summary has been left out for space):
```{r, echo=TRUE}
# Try model with country variable and not region
m_country <- lm(sat_avg ~ ., data=x[,-which(names(x) %in% c("region"))])
summary(m_country)$r.squared
```

Linear model using region (summary has been left out for space):
```{r, echo=TRUE}
# Try model with region variable and not country
m_region <- lm(sat_avg ~ ., data=x[,-which(names(x) %in% c("country"))])
summary(m_region)$r.squared
```

If one looks at the two summaries (Summaries 1 and 2 in the Appendix), one can see that a large amount of the country categories have significant p-values in the first model, and all of the region categories have significant p-values in the second model.  However, the R-squared value for the model using country data was 0.2449, and the R-squared value for the model with the region predictor was 0.1983.  This means that removing some geographical information by condensing the countries into regions caused a drop in R-squared by roughly 19%, which is quite a bit.  As a result, we are going to use the country data in the linear model.

We can also see that train_stat was not significant in either of the models, so let's also remove that from future models.

Lastly, it seems very possible that age and years_teacher are correlated, since teachers that have been teaching for longer are likely to be older.  We can also see that they seem quite correlated from the plot earlier of age against years_teacher.  As a result, we probably don't need both predictors in our linear model.  Let's check the correlation:

```{r}
cor(teacher_data$age, teacher_data$years_teacher, use = "complete.obs")
```

Age and years_teacher are pretty highly correlated (correlation of 0.846). As a result, let's see if there's another way we can use our age and years_teacher data, and combine it into one metric.  Two options are, the age at which the teacher started teaching, and the ratio of years_teacher to age. Perhaps the time in your life when you start teaching may affect how satisfied you are.  For example, perhaps people who start teaching really late are more satisfied in their jobs, as they likely did a lot more research on what the teaching profession was like before making a large career change from their previous job.  It's also possible that the fraction of your life that you have spent teaching, affects how satisfied you are.  

```{r}
teacher_data$age_start <- age_start
teacher_data$age_ratio <- teacher_data$years_teacher/teacher_data$age
```

Let's see whether these new predictors, or one of our older predictors, seems to have the clearest linear relationship with our response variable, sat_avg:

```{r, fig.height=3}
plot1 <- teacher_data %>% ggplot(aes(x=age, y=sat_avg)) + geom_point(alpha=0.05) + labs(title="Age vs Teacher Satisfaction", x="Age (years)", y="Teacher Satisfaction (low to high)")
plot2 <- teacher_data %>% ggplot(aes(x=years_teacher, y=sat_avg)) + geom_point(alpha=0.05) +
  labs(title="Years Spent as a Teacher
       vs Teacher Satisfaction",
       x="Years Spent as a Teacher",
       y="Teacher Satisfaction (low to high)")
plot3 <- teacher_data %>% ggplot(aes(x=age_start, y=sat_avg)) + geom_point(alpha=0.05) +
  labs(title="Age at Start of Teaching Career
       vs Teacher Satisfaction",
       x="Age (years)",
       y="Teacher Satisfaction (low to high)")
plot4 <- teacher_data %>% ggplot(aes(x=age_ratio, y=sat_avg)) + geom_point(alpha=0.05) +
  labs(title="Fraction of Life Spent Teaching
       vs Teacher Satisfaction",
       x="Fraction of Life Spent Teaching",
       y="Teacher Satisfaction (low to high)")
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

From the plots above, it looks like years_teacher has the clearest linear relationship with sat_avg, so we're going to use that predictor in our linear model.

Let's also make sure that the prep_A, prep_B, and prep_C predictors are not too highly correlated.

```{r}
data <- teacher_data[, grep("prep_", names(teacher_data))]
data <- data.frame(prep_A = as.numeric(teacher_data$prep_A),
                   prep_B = as.numeric(teacher_data$prep_B),
                   prep_C = as.numeric(teacher_data$prep_C))
cor(data, use = "complete.obs")
```

We can see from the correlation matrix above that all three prep questions are pretty correlated, and therefore we shouldn't put all three in our linear model.  As a result, let's try creating a single predictor that's the average of all three, in order to indicate the overall feeling that teachers had of how prepared they were.

```{r}
teacher_data$avg_prep <- apply(data, 1, mean)
```

Now let's make a new dataset with all of the variable selections we just made. So we need to remove region, train_stat, age, age_start, age_ratio, and all three prep predictors.

```{r}
x.new <- teacher_data[, -c(sat_indices, na_cols, subject_indices, prep_indices)]
x.new <- na.omit(x.new[, -which(names(x.new) %in% c("train_stat", "region", "age", "age_start", "age_ratio"))])
```

Now let's see if all of the variables that we've decided to use for the linear model, are normally distributed, or if some transformations might be helpful:

We can see from the histogram of sat_avg (Plot 1, displayed in the Dataset section), that although it isn't perfectly normal, it is close enough that it probably doesn't warrant any transformation of the variable. Additionally, after we have a final model, we can check the qqnorm plot to make sure none of the normality assumptions are drastically violated.

```{r}
h1 <- x.new %>% ggplot(aes(x=total_time)) + geom_histogram() +
  labs(title="Histogram of Time Spent on
       Teaching Activities",
       x="Time Spent on Teaching Related Activities",
       y="Count")

h2 <- x.new %>% ggplot(aes(x=years_teacher)) + geom_histogram() +
  labs(title="Histogram of Years Spent
       as a Teacher",
       x="Years of Teaching Experience",
       y="Count")

h3 <- x.new %>% ggplot(aes(x=collab)) + geom_bar() +
  labs(title="Bar Graph of Collaboration Rating",
       x="How Collaborative is School Culture",
       y="Count")

h4 <- x.new %>% ggplot(aes(x=num_subjects)) + geom_bar() +
  labs(title="Histogram of The Number
       of Subjects Taught",
       x="Number Subjects Taught",
       y="Count")

grid.arrange(h1, h2, h3, h4, ncol = 2)
```

Three of the plots above, although not perfectly normal, don't have a huge amount of skew.  In fact, the histogram for time spent on teaching related activities has very little skew.  As a result, we probably don't need to transform any of these variables.  The number of subjects histogram does have a lot of skew.  However, we tried several different transformations, and none of them seemed to help much.  After the creating the model, we will look at the normalqq plot to make sure the normality assumption wasn't violated too significantly.

Model without train_stat (summary removed from report for the sake of space):
```{r, echo=TRUE}
m_final <- lm(sat_avg ~ ., data=x.new)
```

Before analyzing this model, let's make sure that the assumptions for linear regression hold.
```{r, fig.height=3}
par(mfrow=c(1,2))
plot(m_final, which=1)
plot(m_final, which=2)
```

Now let's analyze the coefficients of the model.  We can see from the summary of m_final (Summary 3 in Appendix), that a lot of the coefficients have statistically significant p-values.  However, this is somewhat expected given the large size of our dataset.  In order to decide which predictors seem to be particularly helpful in predicting teacher satisfaction, we will look at the magnitude of the coefficients in the model summary and the confidence intervals of the coefficients (Summary 4 in Appendix) and see how far away they are from zero.

```{r, include=FALSE}
confint(m_final)
```

The largest coefficients in the model in terms of magnitude were: collab.L, countryMEX, countryFIN, countryBFL, countryMYS, and countryNLD.  All the other coefficients had magnitudes that were less than 0.2.

Looking at the confidence intervals of all of the predictors other than country, we can see that the confidence interval for the coefficient of num_subjects is pretty close to zero (it has a range of 0.0017 to 0.0054).  The confidence interval for the coefficient gender is also pretty close to zero, with a range of -0.027 to -0.016.  Two of the confidence intervals closest to zero are for the coefficient of years_teacher (0.00045, 0.00095) and the coefficient for total_time (-0.00063,-0.00031).  Given the small magnitudes of the confidence intervals of these four predictors, they do not seem that significant in predicting teacher satisfaction.

The collab and avg_prep predictors on the other hand, had coefficients with confidence intervals of roughly (0.55, 0.57) and (0.11, 0.12) respectively.  Given that teacher satisfaction was on a scale of 1 to 4, a coefficient of a magnitude around 0.5 seems pretty significant.  A coefficient of a little over 0.1 also seems significant relative to how small many of the other coefficients were.

Lastly, we can see that several of the countries have coefficients with pretty large magnitudes, and confidence intervals for those coefficients that are pretty far from zero.  For example, the confidence interval for the coefficient for countryMEX is (0.4494836350, 0.4990107641) and for countryFIN is (0.2306808115, 0.2792257572). In fact, 15 countries have coefficients with a magnitude of at least 0.1, and a confidence interval for that coefficient that is also at least 0.1 away from 0.  This suggests that country is also significant in predicting teacher satisfaction.

Lastly, let's look at the sign of the coefficients in the model, to see the direction of the relationships between the predictors and sat_avg.  What is positively correlated with sat_avg? What is negatively correlated with sat_avg?  Do these directions make sense?:
TODO

**Conclusion:** The results of this linear model suggest that teachers perceptions of how collaborative the school culture is, as well as how prepared the teachers feel, are significant predictors for how satisfied teachers are.  Interestingly, although how prepared teachers felt was significant in predicting teacher satisfaction, the degree of teacher training was not.  It is possible that this is because it was a Yes or No question on the survey, and therefore we didn't have more information on the quantity or quality of training that teachers received, whereas teachers were able to rate how prepared they felt in several areas on a scale of "Not at all", "Somewhat", "Well", and "Very well".  Lastly, country was very useful in predicting teacher satisfaction.  This could be due to a variety of reasons, including cultural values and attitudes at work, school legislation and restrictions, funding of schools, the ratio of public to private schools, and the respect that the teaching profession receives, all of which vary between countries, and are environmental factors at the country level, rather than the kind of information available in a teacher survey.

### Analysis for Research Question 2

Given how useful country was in predicting teacher_satisfaction, it would be interesting to see whether there are any significant regional differences in some of the other significant predictors from our linear model above.  For example, in the regions with higher teacher satisfaction, are collaboration ratings also higher? 

Therefore, let's now address our second research question, and try to identify the differences across different regions for teacher job satisfaction and other classroom factors.

First, we plot some maps to better visualize the teacher satisfaction across countries and by region.

```{r, warning=FALSE, include=FALSE}
# set to ISO codes
teacher_data$country <- revalue(teacher_data$country, c("AAD"="ARE", "BFL"="BEL","CAB"="CAN","CSH"="CHN","ENG"="GBR"))
# apparently plyr dependencies will mess things up
detach(package:plyr)
country_avg <- teacher_data %>% group_by(country) %>% summarize(mean_satisfaction = mean(sat_avg, na.rm=TRUE))
library(plyr); library(dplyr)
#code for map with average teacher satisfaction by country
sPDF <- joinCountryData2Map(country_avg, joinCode = "ISO3", nameJoinColumn = "country")
```

```{r, fig.height=2.5, fig.width=9}
par(mar=c(0.1, 0.1, 0.1, 0.1))
mapParams <- mapCountryData(sPDF, nameColumnToPlot = "mean_satisfaction", colourPalette="terrain", oceanCol = "lightblue", missingCountryCol="gray", addLegend = FALSE)
do.call( addMapLegend, c(mapParams, legendWidth=0.5, legendShrink=0.5))
```

Now, we plot the regions we categorized countries by, which we found using the World Health Organization data.

*Also I hope this will look better once knitted but if not it will need to be tweaked.Sad!*

```{r, fig.height=3, fig.width=6}
country_region_pairs <- unique(teacher_data[c("country", "region")])
#code for map with average teacher satisfaction by country
sPDF <- joinCountryData2Map(country_region_pairs, joinCode = "ISO3", nameJoinColumn = "country")
mapParams  <-mapCountryData(sPDF, 
               nameColumnToPlot = "region",
               colourPalette=c("#66C2A5", "#FC8D62", "#8DA0CB", "#E78AC3" ,"#A6D854" ,"#FFD92F","#B3B3B3"),
                oceanCol = "lightblue", missingCountryCol="white",
               addLegend = FALSE,
               mapTitle = "WHO Regions"
               )
do.call(addMapLegendBoxes, c(mapParams, x='bottomleft',title="Region", cex=.5))
```

#### Plots

(1) We are first going to plot sat_avg vs region, using the region variable as a factor. This will visually tell us whether there might be a significant difference in satisfaction across regions.

```{r, fig.height=2.5}
## p3, used as an example plot earlier in this write-up, is a plot of sat_avg vs region
## will fix labels once we figure out how big it can be in the final writeup
p3
```

As we stated above, visually, while all median values for sat_avg hover around the range of 2.5 to 3.5, there is still a noticeable difference in some of the distributions (e.g. the median and both quartiles of the satisfaction value for North America is noticeably above that of Eastern Europe). However, we would classify each region's average as "satisfied", as it falls above 2.5

Based on which predictors affected job satisfaction in Part 1, we can see which regions stand out, either positively or negatively, in those factors 

```{r}
# collab plot
p41 <- teacher_data %>% filter(!is.na(collab)) %>% ggplot(aes(x = product(collab, region), fill=factor(collab)), na.rm=TRUE) +  geom_mosaic() + scale_fill_manual(values=c("red3","indianred1", "lightgreen", "green3")) +  theme(axis.text.x=element_text(angle=-25, hjust= .1)) + labs(x="Region", title='How Collaborative is School Culture') + guides(fill=guide_legend(title = "Response", reverse = TRUE))

# prep plot
#p42 <- teacher_data[, c("avg_prep", "prep_B", "prep_C","region")] %>%
#  gather(-region, key = "var", value = "value") %>%
#    mutate(value = factor(value, levels = c("Very Well", "Well", "Somewhat", "Not at all"))) %>%
#  ggplot(aes(x = product(value,region), fill=factor(value))) + geom_mosaic() +
#    theme(axis.text.x=element_text(angle=-25, hjust= .1)) + labs(x="Region", title='How Prepared the #Teacher felt') + guides(fill=guide_legend(title = "Response", reverse=TRUE)) + 
#    facet_wrap(~ var, scales = "free") 

p42 <- teacher_data %>% filter(!is.na(avg_prep)) %>% ggplot(aes(x=reorder(region, -avg_prep, median), y=avg_prep)) + geom_boxplot() + xlab("Region") + ylab("Average Feeling of Preparation")

```

```{r, echo=FALSE, fig.height=4, fig.width=7, message=FALSE, warning=FALSE}
grid.arrange(p41, p42, ncol=2)
```


(3) Given that there appear to be differences by region from the plots above, let's see if these differences are statistically significant using an anova test.  We will then look at which variables from part (2) are significant when predicted with region. 

```{r}
# Check which variables have a significant correlation with region
anova(lm(as.numeric(collab) ~ region, data=teacher_data))
anova(lm(as.numeric(avg_prep) ~ region, data=teacher_data))
```

It looks like all of the variables vary across regions.

Linear model summaries removed from below for space.

```{r, echo=TRUE}
# Collab and region LM
lmcolreg <- lm(sat_avg ~ region*as.numeric(collab), data=teacher_data)
# prep_A and region LM
lmpareg <- lm(sat_avg ~ region*as.numeric(prep_A), data=teacher_data)
# prep_B and region LM
lmpbreg <- lm(sat_avg ~ region*as.numeric(prep_B), data=teacher_data)
# prep_C and region LM
lmpcreg <- lm(sat_avg ~ region*as.numeric(prep_C), data=teacher_data)
```

#### One-way ANOVA Test

After the analysis above is completed, we'll run an ANOVA test to see if there are statistically significant variances in mean `sat_avg` across regions.

```{r, echo=TRUE}
# Regression of sat_avg vs region
msatreg <- lm(sat_avg ~ region, data=teacher_data)

# ANOVA of regression
anova(msatreg)

## insert pairwise t-test with adjustment
```

We will also see if there are statistically significant variances across regions in any of the significant predictors from part (1) that were particularly predictive of teacher satisfaction.

```{r}
# Template code for anova of predictor against region TODO: switch collab with predictive factors after found:
mcollabreg <- lm(collab ~ region, data=teacher_data)
anova(msatreg)
```

Let's see how much of the R-squared in our model from part 1 was due to just the country predictor:
```{r, echo=FALSE}
m_countryonly <- lm(I(sat_avg^2) ~ country, data=x.new)
summary(m_countryonly)$r.squared
```
0.08257/0.2474 is roughly 33.4%, therefore the country variable alone accounts for 33% of the variability that our final model in Part 1 was able to explain.

# Conclusion:

Overall, our models didn't completely explain the variation in the data. We expected real datasets to be messier than the ones we had dealt with in problem sets, and so we were pleased with the observations we did find.

__insert findings here lol__

Before we started looking into the data, we expected there to be a lot more unsatisfied teachers than there actually were. Indeed, if we take `sat_avg` scores of above 2.5 to mean that the respondent is generally satisfied (if on average, they responded Agree or Strongly Agree more than Disagree or Strongly Disgree on the teacher satisfaction questions), then
```{r, echo=TRUE}
nrow(teacher_data[teacher_data$sat_avg >=2.5,])/nrow(teacher_data)
```
of our respondents were satisfied. Of course, this is a very low baseline (it just means the teacher didn't respond "Agree" or "Strongly Agree" to a question like "I regret that I decided to become a teacher.") If we were to collect the data particularly to understand how various factors affect teacher satisfication. we would want to get a more precise sense of how satisfied or unsatisfied teachers are about specific aspects of teaching. We might ask respondents to our survey to rate, on a scale from 1 - 10, their satisfaction with things like working conditions, pay, sense of impact, and more. 

As far as concerns about the data, when we did our data cleaning, we did find some suspicious responses. There were things we could say were impossible, such as working longer than one has been alive, but we could have other incorrectly reported responses that we weren't  able to immediately mark as suspicious. Because all of the data was a self reported survey, inaccurate/falsely reported answers were pretty inevitable, but are still a concern about the data.


One area that our analysis would have benefited from other data sources would be countries. We created our `regions` variable using the World Health Organization regions categories, so that it was easier to visualize and interpret our results by group of countries. However, there are many other ways we could have tried to categorize the countries - for instance, because more collaborative school environments correlated with higher teacher satisfaction, we could have tried to group the countries by their rankings on the individualism vs collectivism scale, to see whether national attitudes made a satistically significant difference.



## Code Appendix

First, here is the data cleaning code from the Data Exploration Assignment


```{r data_cleaning,eval=FALSE}
# Lower-secondary-school teacher data (international file, all countries) is the file titled BTGINTT2
lower_second_data <- read.spss("SPSS_International/BTGINTT2.sav", to.data.frame=TRUE, use.missings=TRUE)

cols <- c("CNTRY", "TT2G01", "TT2G02", "TT2G05B", "TT2G11", "TT2G13A", "TT2G13B", "TT2G13C", "TT2G16", "TT2G30G", "TT2G38", "TT2G44E", "TJOBSATS")

# We are creating a dataset with the columns from our project proposal, the 10 questions related to job satisfaction, which are columns TT2G46A through TT2G46J, as well as a few additional columns which we also think might be useful
data <- cbind(lower_second_data[,cols],lower_second_data[,grep("TT2G46", colnames(lower_second_data))], lower_second_data[,grep("TT2G15", colnames(lower_second_data))])

# Rename some of the columns so that they're easier to work with
data <- data %>% rename(gender=TT2G01, age=TT2G02, country=CNTRY, years_teacher = TT2G05B, train_stat = TT2G11, prep_A=TT2G13A, prep_B=TT2G13B, prep_C=TT2G13C, total_time=TT2G16, feedback_salary=TT2G30G, collab=TT2G44E, num_students=TT2G38, satisfaction=TJOBSATS)

# Columns that start with TT2G15 correspond to several questions asking what subjects teachers teach:
#Let's manipulate the subject data so that it's in a form that's useful to us

# First make an ID column to merge back after isolating the subject as one column
data$ID <- seq.int(nrow(data))

# Indices of columns dealing with subject taught
indices <- grep("TT2G15", colnames(data))[-1] #-1 to deal with the TT2G15 column that isn't for a specific subject

# Convert "Yes" "No" levels to 0 for didn't teach subject and 1 for taught subject
for (i in indices) {
  data[,i] <- 2 - as.numeric(data[,i])
}

# Rename columns by subject
data <- data %>% rename(R_WR_Lit=TT2G15A, Math=TT2G15B, Science=TT2G15C, Soc_Studies = TT2G15D, For_Lang = TT2G15E, Gk_Ln=TT2G15F, Tech=TT2G15G, Arts=TT2G15H, PE=TT2G15I, Rel_Ethics=TT2G15J, Practical=TT2G15K, Other=TT2G15L)

# Names of columns 
col_names <- colnames(data)[indices]

# Below is just the R code for extracting the subject column
# This is separated for clarity since it is extensive in and of itself

df <- data[25:37]

# Melt to format -- similar to "gather"
temp_by_subj <- melt(df,id="ID")

# Only keep 1's (subjects taught)
temp_by_subj <- temp_by_subj[which(temp_by_subj$value==1),]

# Merge the subject table in with data
data_by_subject <- merge(data,temp_by_subj,all.y=T, by = "ID")

# Clean up unnecessary columns
data_by_subject <- data_by_subject[,-(c(15:24,26:37,39))]
data_by_subject <- data_by_subject %>% rename(Subject=variable)

# Below is the code used to clean up the satisfaction related questions, as well as a few other columns

# All ten columns with responses related to job satisfaction are factors, with 4 levels: "Strongly Disagree", "Disagree", "Agree", "Strongly Agree".
# We are going to convert these 10 columns to numerical data, where the factors get converted to the integers 1, 2, 3, and 4, with 1 corresponding to Strongly Disagree and 4 corresponding to Strongly Agree.
for (i in grep("TT2G46", colnames(data))) {
  data[,i] <- as.numeric(data[,i])
}

# Questions C, D, and F were negatively phrased questions, so we are reversing their scales so we can compare them to the positively phrased questions.
data$TT2G46C <- 5 - data$TT2G46C
data$TT2G46D <- 5 - data$TT2G46D
data$TT2G46F <- 5 - data$TT2G46F

# Questions TT2G46A through J were all related to teacher satisfaction.  Consequently, we are going to create a column that averages the results of these 10 questions to get a sense of teacher satisfaction
data$TT2G46_avg <- rowMeans(data[,grep("TT2G46", colnames(data))], na.rm=TRUE)

# Clean up country column, which has a lot of whitespace
data$country <- as.factor(trimws(as.character(data$country)))

# Clean up train_stat variable so it's easier to work with
# Make No the first factor, so that if we convert it to numeric, it's easy to have 0 represent No and 1 represent Yes
data$train_stat <- relevel(data$train_stat, "No")
```


*note, we need to figure out how many code chunks in final, and then remove the last one here, because otherwise the datacleaning code will be printed twice 
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```
