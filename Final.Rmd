---
title: "Final Project""
author: "Veena Advani, Annie Chen, Robert Tung"
date: "Due 4/28/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library("foreign")
library("dplyr")
library("tidyr")
library("countrycode")
library("rworldmap")
library(ggplot2)
library(reshape2)
library(FNN)
library(grid)
library(gridExtra)
library(leaps)
```

```{r, include=FALSE}
## NOTE! THIS RUNS AND DOES NOT SHOW IN KNITTED
# To start, let's read in the cleaned datasets from our last submission:
teacher_data <- read.csv("TALIS.csv")
subject_data <- read.csv("subject_data.csv", as.is=TRUE)

# Change column names of 10 satisfaction questions to make more readable
satcols <- grep("TT2G46", colnames(teacher_data))
indices <- satcols[satcols != which(colnames(teacher_data) == "TT2G46_avg")]
colnames(teacher_data)[indices] <- c("sat1", "sat2", "sat3", "sat4", "sat5", "sat6", "sat7", "sat8", "sat9", "sat10")
names(teacher_data)[names(teacher_data)=="TT2G46_avg"] <- "sat_avg"

# Column "TT2G15" contains no useful data, since it's all NAs, so we're going to remove it
# Column "ID" is no longer necessary, was used for making the subject dataset
teacher_data <- teacher_data[,-which(names(teacher_data) %in% c("TT2G15","ID"))]

# Change some of the subject names (one of the comments in our dataset submission was that they weren't clear):
names(teacher_data)[names(teacher_data) == "For_Lang"] <- "Modern_Lang"
names(teacher_data)[names(teacher_data) == "Gk_Ln"] <- "Ancient_Lang"

# We also thought it might be useful to have a variable that indicates the number of subjects a given teacher teaches, so let's add it below:
subjcols <- c(24:35)
teacher_data$num_subjects <- rowSums(teacher_data[,subjcols], na.rm=TRUE)

# We discussed in our dataset submission why we weren't going to use their teacher satisfaction variable, and were instead going to use the average of the 10 satisfaction questions:
teacher_data <- teacher_data[, -which(names(teacher_data) %in% c("satisfaction"))]

# A lot of the variables stored as categorical variables with the categories "Strongly Disagree", "Disagree", "Strongly Agree", and "Agree", are currently factors.  Given that these categories do have a natural ordering, we're going to convert them into ordered factors.
teacher_data$prep_A <- factor(teacher_data$prep_A, levels=c("Not at all", "Somewhat", "Well", "Very well"), ordered=TRUE)
teacher_data$prep_B <- factor(teacher_data$prep_B, levels=c("Not at all", "Somewhat", "Well", "Very well"), ordered=TRUE)
teacher_data$prep_C <- factor(teacher_data$prep_C, levels=c("Not at all", "Somewhat", "Well", "Very well"), ordered=TRUE)
teacher_data$feedback_salary <- factor(teacher_data$feedback_salary, levels=c("No positive change", "A small change", "A moderate change", "A large change"), ordered=TRUE)
teacher_data$collab <- factor(teacher_data$collab, levels=c("Strongly disagree", "Disagree", "Agree", "Strongly agree"), ordered=TRUE)

# We'll split the country data into regions using the World Health Organization  categories, so that we can try to find regional patterns

# Brazil, Chile
teacher_data$region[teacher_data$country %in% c("BRA", "CHL")] <- "South America"

# Canada, Mexico, United States
teacher_data$region[teacher_data$country %in% c("CAB", "MEX", "USA")] <- "North America"

#  Bulgaria, Czech Republic,  Poland,  Romania, Russia, Slovak Republic
teacher_data$region[teacher_data$country %in% 
                        c("BGR", "CZE", "POL", "ROU", "RUS", "SVK")] <- "Eastern Europe"

# Denmark, England, Estonia, Finland,   Latvia,  Norway, Sweden
teacher_data$region[teacher_data$country %in% 
                        c("DNK", "ENG",  "EST", "FIN", "LVA", "NOR", "SWE")] <- "Northern Europe"

# Spain, Croatia, Italy, Latvia,  Portugal,  Serbia, 
teacher_data$region[teacher_data$country %in% 
                        c("ESP", "HRV", "ITA", "LVA", "PRT", "SRB")] <- "Southern Europe"

# Belgium,  France, Netherlands
teacher_data$region[teacher_data$country %in% 
                        c("BFL", "FRA","NLD")] <- "Western Europe"

# Abu Dhabi, Georgia, Israel                     
teacher_data$region[teacher_data$country %in% c("AAD", "GEO", "ISR")] <- "East Mediterranean"

# Australia, China, Japan, South Korea, Malaysia, New Zealand, Singapore
teacher_data$region[teacher_data$country %in% c("AUS", "CSH", "JPN", "KOR", "MYS", "NZL", "SGP")] <- "Western Pacific"
teacher_data$region <- as.factor(teacher_data$region)
```
# Introduction

This analysis will focus on the Teaching and Learning International Survey (TALIS) Dataset.
Some information about TALIS:

* It aimed to collect information on teaching profession, working conditions, and learning environments of schools
* One goal was to come up with metrics and information that would enable comparisons across countries
* It focused on SCED* Level 2 schools (lower secondary schools)
* At selected schools, the principal (or head administrator) and a random selection of up to 22 teachers were chosen to complete voluntary online questionnaires
* There are 36 countries in the dataset (some of which are non-English speaking countries). The only country requirement was that participating countries must be industrialized.

#### Main Research Questions:
(1) Which variables and classroom factors are the best predictors of teacher job satisfaction?
(2) How do teacher satisfaction, as well as some of the significant predictors found from question 1, vary by country?

#### Motivation of Study:
* Education is important to any country's basic development and functioning
* There is a fair amount of diversity across countries with regard to how well the teaching profession is respected, as well as how much countries fund their education systems.
* As a result, teacher working conditions vary quite a bit
* However, in order to make sure that there are enough teachers, and that they are able to teach their students well, it is important that teacher satisfaction is at a high enough level.  If teacher satisfaction is too low, it becomes difficult to retain a large enough teaching force
* Therefore, understanding how teacher satisfaction varies across the 36 countries in the data set, and which aspects of a school and environment correlate most with teacher satisfaction, could be helpful information in order to make sure that countries have enough teachers, and that teachers have enough resources to do their jobs well.

# Data

Each row of the dataset is for a single teacher's response to the questionnaire (There are 117876 rows).  Each column in the dataset corresponds to either a particular question in the questionnaire, or a summary variable for a section of the questionnaire.

The variables we are using are:

* `country (CNTRY):` Country of teacher (factor variable, 36 levels)
* `region:` Region the teacher's country is in (factor variable, 8 levels)
* `gender (TT2G01):` Teacher Gender (factor variable, 2 levels)
* `age (TT2G02):` Teacher Age (numeric variable)
* `years_teacher (TT2G05B):` Years of teaching experience (numeric variable)
* `train_stat (TT2G11):` Training level of teacher (factor variable)
* `prep_A/B/C (TT2G13A/B/C):` How prepared the teacher felt (factor variable, 4 levels)
* `total_time (TT2G16):` Total time spent on teaching related activities in hours per week (numeric variable)
* `feedback_salary (TT2G30G):` Whether performance feedback influences salary (factor variable, 4 levels)
* `collab (TT2G44E):` How collaborative is the school culture? (factor variable, 4 levels)
* `num_students (TT2G38):` Number of students in the teacherâ€™s class (numeric variable)
* `satisfaction (TJOBSATS):` Self rated teacher job satisfaction (factor variable, 4 levels)
* `Various Subject Names (TT2G15[A - L]):` 12 columns representing which subjects the teacher teaches (boolean numeric variable)
* `num_subjects:` The number of subjects the teacher teaches (numeric variable)
* `sat1-sat10 (TT2G46[A - J]):` Ten questions related to teacher job satisfaction (factor variables, 4 levels)
* `sat_avg:` An average of the ten questions related to teacher job satisfaction (numeric variable)

# Analysis

#### Initial Plots Describing Data

Below are three plots that show our data and hopefully motivate our research questions and the analysis.
```{r, warning=FALSE, echo=FALSE}
# Histogram of teacher satisfaction
p1 <- teacher_data %>% ggplot(aes(x=sat_avg)) + geom_histogram(binwidth = 0.2) + labs(title="Plot 1: Histogram of Teacher Satisfaction", x="Teacher Satisfaction (low to high)", y="Count")

# Boxplot of whether they consider their school a collaborative environment to satisfaction
p2 <- teacher_data %>% filter(!is.na(collab)) %>% ggplot(aes(x=collab, y=sat_avg)) + geom_boxplot() +
  labs(title="Plot 2: Relationship Between Collaboration Level
       of the School and Teacher Satisfaction",
       x="Response to 'There is a collaborative school culture
       which is characterised by mutual support'",
       y="Teacher Satisfaction")

# Boxplot of region to satisfaction
p3 <- teacher_data %>% filter(!is.na(sat_avg)) %>% ggplot(aes(x=reorder(region, -sat_avg, median), y=sat_avg)) + geom_boxplot() + labs(title="Plot 3: Teacher Satisfaction by Region", x="Region", y="Teacher Satisfaction")
```

```{r, echo=FALSE, fig.height=2.25, fig.width=10, message=FALSE, warning=FALSE}
grid.arrange(p1, p2, ncol=2)
```

```{r, echo=FALSE, fig.height=2.25, fig.width=9}
p3
```

**Plot 1** provides a full histogram of teacher satisfaction. As we are later analyzing the effect of different variables on teacher satisfaction, and the variation between different regions, it is important to note the overall distribution of the data. 

**Plot 2** provides a box plot of collab vs sat_avg. This plot is an example of one we would find in regard to our first research question, in which we assess which variables are particularly predictive of overall teacher satisfaction. In looking at this particular plot, we can actually see a strong visual trend between how collaborative a teacher viewed his/her school environment to be, and how satisfied that teacher was. We will provide similar plots for other variables in the analysis of our first research question, as well as more rigorous analysis to be described below.

**Plot 3** provides a box plot of geographic region vs sat_avg. This is relevant to our second research question, which revolves around variations in sat_avg and other variables across different geographical regions. Visually, while all median values for sat_avg hover around the range of 2.5 to 3.5, there is still a noticeable difference in some of the distributions (e.g. the median and both quartiles of the satisfaction value for North America is noticeably above that of Eastern Europe). We will explore additional plots like this one, but with other variables, in our full analysis.

### Analysis for Research Question 1

To address our first research question, and to try to identify which variables are the best indicators of teacher job satisfaction, we are going to attempt to use several techniques.

#### Linear Regression Analysis
(1) Before creating a linear regression model, let's first plot our data and see what relations there appear to be between the variables.
```{r}
ggpairs(teacher_data, cardinality_threshold=NULL)
```

(1) We are first going to run two linear regression models: one with all of the predictors in the dataset, and one with all of the predictors that have NA values in less than 10% of observations.
(2) We are going to use forward, backward and bidirectional stepwise selection, and look at the linear models that end up getting chosen from those three methods.
(3) We will then look at which predictors appeared to be the most significant in the linear models from (1) and (2).  

```{r}
# In order to have easier control over which predictors to use in our model, below we create several lists of indices that we might want to include or exclude as a group

# We don't want to use the 10 satisfaction questions to predict satisfaction, so let's create a list of the indices we need to avoid
satcols <- grep("sat", colnames(teacher_data))
sat_indices <- satcols[satcols != which(colnames(teacher_data) == "sat_avg")]

# Some columns also have many more NA values, lets make a list of those so we can choose whether or not to include them in our linear models
na_cols <- c()
for(i in 1:ncol(teacher_data)) {
  if(sum(is.na(teacher_data[,i]))/nrow(teacher_data) > 0.10) {
    na_cols <- c(na_cols,i)
  }
}

subject_indices <- c(23:34)

# For the linear model with most of the predictors, let's remove the columns with a large number of NA values.  
# Let's also remove the columns that correspond to the 10 satisfaction questions, since they were used to create the sat_avg variable (our response variable)
x <- na.omit(teacher_data[, -c(sat_indices, na_cols, subject_indices)])
# Try model with country variable and not region
m1 <- lm(sat_avg ~ ., data=x[,-which(names(x) %in% c("region"))])
# Try model with region variable and not country
m2 <- lm(I(sat_avg^2) ~ ., data=x[,-which(names(x) %in% c("region"))])
summary(m1)
summary(m2)
```

Let's make sure that the assumptions for linear regression hold.

Using stepwise regression
```{r}
# remove SAT questions, since they obviously explain our predictor variable, and then remove NAs for stepwise regression
#also remove subject columns, we have a column that counts the number of subjects each teacher is teaching, to represent this information instead.
m.full <- lm(sat_avg ~., data=x[,-13])
m.empty <- lm(sat_avg ~ 1, data=x)
```

Forward, backwards, and both
```{r}
#with AIC
m.forward <- step(m.empty, scope=list(upper=m.full),
                  data=x, direction="forward", trace=FALSE)
m.backward <- step(m.full, scope=list(lower=m.empty),
                  data=x, direction="backward", trace=FALSE)
m.both <- step(m.empty, scope=list(upper=m.full),
                  data=x, direction="both", trace=FALSE)

#with BIC
m.forward <- step(m.empty, scope=list(upper=m.full),
                  data=x, direction="forward", trace=FALSE, k=log(nrow(x)))
m.backward <- step(m.full, scope=list(lower=m.empty),
                  data=x, direction="backward", trace=FALSE, k=log(nrow(x)))
m.both <- step(m.empty, scope=list(upper=m.full),
                  data=x, direction="both", trace=FALSE, k=log(nrow(x)))
```

We can see from the results above, that with AIC as the criterion, all three stepwise models resulted in using the same 9 predictors, which were: collab, country, prep_A, prep_B, prep_C, gender, age, total_time, and years_teacher.

Using the BIC criterion, all three models ended up using all of the same predictors as the AIC stepwise models, except without prep_A.

```{r}
#m3 <- regsubsets(sat_avg ~ ., data=x, nbest=1, method="exhaustive")
```


#### One-way ANOVA Test

Based on the predictors that were significant from part (3) above, we will perform one-way ANOVA tests with each the categorical predictors to see if there are statistically different variances in mean `sat_avg` across different categories.  We will also graph these variables against our job satisfaction (sat_avg) variable to get a better idea of the relationship between those predictors and our response variable.  Based on the analysis we've done so far, some potential candidates for predictors are collab, prep_C, and region.

#### Logistic Regression Analysis

In addition to using linear regression, we will also then convert our job satisfaction variable into a categorical variable with 2 categories, "satisfied" and "unsatisfied".  We will then see if Logistic Regression yields different results than our Linear Regression approach.

For Logistic Regression we will:

(1) Try a full model, with all the predictors.
(2) Try forward, backward and bidirectional stepwise selection.

Then we will analyze the models from (1) and (2) to try to determine which columns are the best predictors of whether or not teachers are satisfied.  We will then compare these results to the results we got from linear regression.

```{r}
teacher_data$isSatisfied <- teacher_data$sat_avg > 2.5
# remove NA rows, since they don't contain meaningful information for us.
teacher_data <- teacher_data[!is.na(teacher_data$sat_avg),]
# remove SAT questions, since they obviously explain our predictor variable
sat_indices <- grep("sat", names(teacher_data))[-12]
subject_indices <- c(23:34)
teacher_data <- teacher_data[, -c(sat_indices, subject_indices)]
# don't use country
teacher_data <- teacher_data[,-1]
# split into train and test
set.seed(365)
train_indices <- sample(nrow(teacher_data), .8 * nrow(teacher_data))
train <- na.omit(teacher_data[train_indices,])
test <- teacher_data[-train_indices,]

m1 <- glm(isSatisfied ~., data=train)
p1.preds <- ifelse(predict(m1, newdata=test, type="response") > 0.5,1,0)
mean(p1.preds != test$isSatisfied, na.rm = TRUE)
# see what our misclassification rate would be if we asigned randomly
fake <- sample(test$isSatisfied, length(test$isSatisfied), replace=TRUE)
mean(fake != test$isSatisfied, na.rm = TRUE)

# knn.preds <- knn(train[,1:13], na.omit(test[,1:13]), train$isSatisfied, k=10)
```

### Analysis for Research Question 2

To address our second research question, and try to identify the differences across different regions for teacher job satisfaction, we are going to try several techniques.

#### Plots

(1) We are first going to plot sat_avg vs region, using the region variable as a factor. This will visually tell us whether there might be a significant difference in satisfaction across regions.

```{r}
## p3, used as an example plot earlier in this write-up, is a plot of sat_avg vs region
p3
```

(2) We are going to plot each of the predictive factors that were found to be significant in Research Question 1 against region, to see if perhaps the region is affecting these variables and thus affecting satisfaction indirectly. These variables are collab, country, prep_A, prep_B, prep_C, gender, age, total_time, and years_teacher. We will not, however, talk about country because of the obvious correlation between that and region. We will also not talk about gender, as this is not a scale in the dataset.

```{r}
# Collab plot
teacher_data %>% filter(!is.na(collab)) %>% ggplot(aes(x=reorder(region, -as.numeric(collab), median), y=as.numeric(collab))) + geom_boxplot() + xlab("Region") + ylab("Collab")

# prep_A plot
teacher_data %>% filter(!is.na(prep_A)) %>% ggplot(aes(x=reorder(region, -as.numeric(prep_A), median), y=as.numeric(prep_A))) + geom_boxplot() + xlab("Region") + ylab("prep_A")

# prep_B plot
teacher_data %>% filter(!is.na(prep_B)) %>% ggplot(aes(x=reorder(region, -as.numeric(prep_B), median), y=as.numeric(prep_B))) + geom_boxplot() + xlab("Region") + ylab("prep_B")

# prep_C plot
teacher_data %>% filter(!is.na(prep_C)) %>% ggplot(aes(x=reorder(region, -as.numeric(prep_C), median), y=as.numeric(prep_C))) + geom_boxplot() + xlab("Region") + ylab("prep_C")

# age plot
teacher_data %>% filter(!is.na(age)) %>% ggplot(aes(x=reorder(region, -as.numeric(age), median), y=as.numeric(age))) + geom_boxplot() + xlab("Region") + ylab("Age")

# total_time plot
teacher_data %>% filter(!is.na(total_time)) %>% ggplot(aes(x=reorder(region, -as.numeric(total_time), median), y=as.numeric(total_time))) + geom_boxplot() + xlab("Region") + ylab("Total Time Working")

# years_teacher plot
teacher_data %>% filter(!is.na(years_teacher)) %>% ggplot(aes(x=reorder(region, -as.numeric(years_teacher), median), y=as.numeric(years_teacher))) + geom_boxplot() + xlab("Region") + ylab("Years Working as Teacher")
```

(3) We will then look at which variables from part (2) are significant when predicted with region. If any are statistically significant, we will graph sat_avg as a function of that variable for each region.

```{r}
# Check which variables have a significant correlation with region
anova(lm(as.numeric(collab) ~ region, data=teacher_data))
anova(lm(as.numeric(prep_A) ~ region, data=teacher_data))
anova(lm(as.numeric(prep_B) ~ region, data=teacher_data))
anova(lm(as.numeric(prep_C) ~ region, data=teacher_data))
anova(lm(as.numeric(age) ~ region, data=teacher_data))
anova(lm(as.numeric(total_time) ~ region, data=teacher_data))
anova(lm(as.numeric(years_teacher) ~ region, data=teacher_data))
```

It looks like all of the variables vary across regions.

```{r}
# Template code for regression of (3) TODO: switch collab with predictive factors after found:
summary(lm(sat_avg ~ region*as.numeric(collab), data=teacher_data))

# Plot code for (3) TODO: switch collab with predictive factors after found:
teacher_data %>% filter(!is.na(as.numeric(collab))) %>% ggplot(aes(x=as.numeric(collab), y=as.numeric(sat_avg))) + geom_point(alpha=0.05) + xlab("Collab") + ylab("Sat") + facet_wrap(~region)
```

(1) Indeed, at least visually, there seems to be some discrepancy in the level of teacher satisfaction across regions (e.g. the median and both quartiles of Teacher Satisfaction for North America are higher than those of Eastern Europe).

(2) TODO
(3) TODO

#### One-way ANOVA Test

After the analysis above is completed, we'll run an ANOVA test to see if there are statistically significant variances in mean `sat_avg` across regions.

```{r}
# Regression of sat_avg vs region
msatreg <- lm(sat_avg ~ region, data=teacher_data)
summary(msatreg)

# ANOVA of regression
anova(msatreg)
```

We will also see if there are statistically significant variances across regions in any of the significant predictors from part (1) that were particularly predictive of teacher satisfaction.

```{r}
# Template code for anova of predictor against region TODO: switch collab with predictive factors after found:
mcollabreg <- lm(collab ~ region, data=teacher_data)
anova(msatreg)
```
